{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6SWbHZiixjLM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os\n",
        "from collections import Counter\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "import random\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.modules import dropout\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from os import path\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from collections import defaultdict\n",
        "# import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHziLFFkxpLo",
        "outputId": "6384bd40-73d3-4bf1-df7b-191ae77cc8a4"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Tt0FuTgxHgR",
        "outputId": "ca89ae68-18a9-4b37-b25d-ef2c9f562373"
      },
      "outputs": [],
      "source": [
        "# !pip install tsai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "azkdIjrRCHmW"
      },
      "outputs": [],
      "source": [
        "# from tsai.all import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAlipBRW0Iw6",
        "outputId": "1c3fa258-3779-46e8-ae00-143ed85cbbc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n"
          ]
        }
      ],
      "source": [
        "flag_cuda = torch.cuda.is_available()\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "device= None \n",
        "if not flag_cuda:\n",
        "    print('Using CPU')\n",
        "else:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print('Using GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BqAuigAVri4"
      },
      "source": [
        "# Datasplit generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GTasUPA7xqrl"
      },
      "outputs": [],
      "source": [
        "# PATH = \"dataset/ECE542_sp2022_Project_TerrainRecognition/\"  #'/content/drive/MyDrive/ECE542/CompetitiveProject/ECE542_sp2022_Project_TerrainRecognition'\n",
        "# TRAIN_PATH = PATH +'/TrainingData/'\n",
        "# TEST_PATH = PATH + \"./TestData/\"\n",
        "# val_subject = 6\n",
        "# # test_subject = 8\n",
        "\n",
        "S_PATH = \"dataset/ECE542_sp2022_Project_TerrainRecognition/\"  #'/content/drive/MyDrive/ECE542/CompetitiveProject/ECE542_sp2022_Project_TerrainRecognition'\n",
        "PATH = S_PATH +'/TrainingData/'\n",
        "TEST_PATH = S_PATH + \"./TestData/\"\n",
        "val_subject = 6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cKd2UQXI0JRb"
      },
      "outputs": [],
      "source": [
        "files = []\n",
        "for filename in os.listdir(PATH):\n",
        "    files.append(filename)\n",
        "files = sorted(files, key = lambda x: (int(x.split('_')[1]),int(x.split('_')[2]), x.split('_')[4] ))\n",
        "files_train = list(filter(lambda x: int(x.split('_')[1]) not in  [val_subject], files))\n",
        "files_test = list(filter(lambda x: int(x.split('_')[1]) == val_subject, files))\n",
        "# files_test = list(filter(lambda x: int(x.split('_')[1])== test_subject, files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YfXWEtenccl",
        "outputId": "ee524b86-4c4b-4697-83db-fd48db551982"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['subject_006_01__x_time.csv',\n",
              " 'subject_006_01__x.csv',\n",
              " 'subject_006_01__y_time.csv',\n",
              " 'subject_006_01__y.csv',\n",
              " 'subject_006_02__x_time.csv',\n",
              " 'subject_006_02__x.csv',\n",
              " 'subject_006_02__y_time.csv',\n",
              " 'subject_006_02__y.csv',\n",
              " 'subject_006_03__x_time.csv',\n",
              " 'subject_006_03__x.csv',\n",
              " 'subject_006_03__y_time.csv',\n",
              " 'subject_006_03__y.csv']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "files_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vFuONoZzVyRR"
      },
      "outputs": [],
      "source": [
        "files_train_y = files_train[3::4]\n",
        "# files_train_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nRAyaADMWF3J"
      },
      "outputs": [],
      "source": [
        "files_y_val = pd.concat([pd.read_csv(PATH + i , header=None) for i in files_train_y])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8glK8GBIYMV-",
        "outputId": "c5c6d7cb-e0ba-496b-ba56-c8e7938d5111"
      },
      "outputs": [],
      "source": [
        "# continue_loop = True\n",
        "# threshold = 0.005\n",
        "# _, files_split_dist = np.unique(files_y_val, return_counts=True)\n",
        "# files_split_dist = np.divide(files_split_dist, np.max(files_split_dist))\n",
        "# while continue_loop:\n",
        "#   files_train_y_split_1 = random.sample(files_train_y, int(len(files_train_y) * 0.8))\n",
        "#   files_train_y_split_2 = [i for i in files_train_y if i not in files_train_y_split_1]\n",
        "#   files_split_1_val = pd.concat([pd.read_csv(PATH + i , header=None) for i in files_train_y_split_1])\n",
        "#   files_split_2_val = pd.concat([pd.read_csv(PATH + i , header=None) for i in files_train_y_split_2])\n",
        "#   _, files_split_1_dist = np.unique(files_split_1_val, return_counts=True)\n",
        "#   files_split_1_dist = np.divide(files_split_1_dist, np.max(files_split_1_dist))\n",
        "#   _, files_split_2_dist = np.unique(files_split_2_val, return_counts=True)\n",
        "#   files_split_2_dist = np.divide(files_split_2_dist, np.max(files_split_2_dist))\n",
        "#   if ((files_split_dist[0] - threshold <= files_split_1_dist[0] <= files_split_dist[0] + threshold) and\n",
        "#       (files_split_dist[1] - threshold <= files_split_1_dist[1] <= files_split_dist[1] + threshold) and\n",
        "#       (files_split_dist[2] - threshold <= files_split_1_dist[2] <= files_split_dist[2] + threshold) and\n",
        "#       (files_split_dist[3] - threshold <= files_split_1_dist[3] <= files_split_dist[3] + threshold) and\n",
        "#       (files_split_dist[0] - threshold <= files_split_2_dist[0] <= files_split_dist[0] + threshold) and\n",
        "#       (files_split_dist[1] - threshold <= files_split_2_dist[1] <= files_split_dist[1] + threshold) and\n",
        "#       (files_split_dist[2] - threshold <= files_split_2_dist[2] <= files_split_dist[2] + threshold) and\n",
        "#       (files_split_dist[3] - threshold <= files_split_2_dist[3] <= files_split_dist[3] + threshold)):\n",
        "#           print(files_split_1_dist, files_split_2_dist, files_split_dist)\n",
        "#           break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 136793727\n",
        "random.seed(seed)\n",
        "indices = np.random.choice(len(files_train_y), int(len(files_train_y) * 0.8), replace=False)\n",
        "files_train_y_split_1 = [files_train_y[i] for i in indices]\n",
        "files_train_y_split_2 = [i for i in files_train_y if i not in files_train_y_split_1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "y3Q6ufK4Ylq4"
      },
      "outputs": [],
      "source": [
        "files_train_y_split_1_names = [\"_\".join(i.split(\"__\")[: -1]) for i in files_train_y_split_1]\n",
        "files_train_y_split_2_names = [\"_\".join(i.split(\"__\")[: -1]) for i in files_train_y_split_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fylBPwbDOQWR"
      },
      "outputs": [],
      "source": [
        "training_files = [i for i in files_train if \"_\".join(i.split(\"__\")[: -1]) in files_train_y_split_1_names]\n",
        "validation_files = [i for i in files_train if \"_\".join(i.split(\"__\")[: -1]) in files_train_y_split_2_names]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7chGeUhngfA",
        "outputId": "f5457f6b-7c39-4d92-da48-829f7d14187f"
      },
      "outputs": [],
      "source": [
        "# training_files.extend(files_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(80,\n",
              " ['subject_001_01__x_time.csv',\n",
              "  'subject_001_01__x.csv',\n",
              "  'subject_001_01__y_time.csv',\n",
              "  'subject_001_01__y.csv',\n",
              "  'subject_001_02__x_time.csv',\n",
              "  'subject_001_02__x.csv',\n",
              "  'subject_001_02__y_time.csv',\n",
              "  'subject_001_02__y.csv',\n",
              "  'subject_001_03__x_time.csv',\n",
              "  'subject_001_03__x.csv',\n",
              "  'subject_001_03__y_time.csv',\n",
              "  'subject_001_03__y.csv',\n",
              "  'subject_001_04__x_time.csv',\n",
              "  'subject_001_04__x.csv',\n",
              "  'subject_001_04__y_time.csv',\n",
              "  'subject_001_04__y.csv',\n",
              "  'subject_001_05__x_time.csv',\n",
              "  'subject_001_05__x.csv',\n",
              "  'subject_001_05__y_time.csv',\n",
              "  'subject_001_05__y.csv',\n",
              "  'subject_001_06__x_time.csv',\n",
              "  'subject_001_06__x.csv',\n",
              "  'subject_001_06__y_time.csv',\n",
              "  'subject_001_06__y.csv',\n",
              "  'subject_002_01__x_time.csv',\n",
              "  'subject_002_01__x.csv',\n",
              "  'subject_002_01__y_time.csv',\n",
              "  'subject_002_01__y.csv',\n",
              "  'subject_002_02__x_time.csv',\n",
              "  'subject_002_02__x.csv',\n",
              "  'subject_002_02__y_time.csv',\n",
              "  'subject_002_02__y.csv',\n",
              "  'subject_002_04__x_time.csv',\n",
              "  'subject_002_04__x.csv',\n",
              "  'subject_002_04__y_time.csv',\n",
              "  'subject_002_04__y.csv',\n",
              "  'subject_002_05__x_time.csv',\n",
              "  'subject_002_05__x.csv',\n",
              "  'subject_002_05__y_time.csv',\n",
              "  'subject_002_05__y.csv',\n",
              "  'subject_003_01__x_time.csv',\n",
              "  'subject_003_01__x.csv',\n",
              "  'subject_003_01__y_time.csv',\n",
              "  'subject_003_01__y.csv',\n",
              "  'subject_003_03__x_time.csv',\n",
              "  'subject_003_03__x.csv',\n",
              "  'subject_003_03__y_time.csv',\n",
              "  'subject_003_03__y.csv',\n",
              "  'subject_004_01__x_time.csv',\n",
              "  'subject_004_01__x.csv',\n",
              "  'subject_004_01__y_time.csv',\n",
              "  'subject_004_01__y.csv',\n",
              "  'subject_004_02__x_time.csv',\n",
              "  'subject_004_02__x.csv',\n",
              "  'subject_004_02__y_time.csv',\n",
              "  'subject_004_02__y.csv',\n",
              "  'subject_005_01__x_time.csv',\n",
              "  'subject_005_01__x.csv',\n",
              "  'subject_005_01__y_time.csv',\n",
              "  'subject_005_01__y.csv',\n",
              "  'subject_007_01__x_time.csv',\n",
              "  'subject_007_01__x.csv',\n",
              "  'subject_007_01__y_time.csv',\n",
              "  'subject_007_01__y.csv',\n",
              "  'subject_007_02__x_time.csv',\n",
              "  'subject_007_02__x.csv',\n",
              "  'subject_007_02__y_time.csv',\n",
              "  'subject_007_02__y.csv',\n",
              "  'subject_007_03__x_time.csv',\n",
              "  'subject_007_03__x.csv',\n",
              "  'subject_007_03__y_time.csv',\n",
              "  'subject_007_03__y.csv',\n",
              "  'subject_007_04__x_time.csv',\n",
              "  'subject_007_04__x.csv',\n",
              "  'subject_007_04__y_time.csv',\n",
              "  'subject_007_04__y.csv',\n",
              "  'subject_008_01__x_time.csv',\n",
              "  'subject_008_01__x.csv',\n",
              "  'subject_008_01__y_time.csv',\n",
              "  'subject_008_01__y.csv'],\n",
              " ['subject_001_07__x_time.csv',\n",
              "  'subject_001_07__x.csv',\n",
              "  'subject_001_07__y_time.csv',\n",
              "  'subject_001_07__y.csv',\n",
              "  'subject_001_08__x_time.csv',\n",
              "  'subject_001_08__x.csv',\n",
              "  'subject_001_08__y_time.csv',\n",
              "  'subject_001_08__y.csv',\n",
              "  'subject_002_03__x_time.csv',\n",
              "  'subject_002_03__x.csv',\n",
              "  'subject_002_03__y_time.csv',\n",
              "  'subject_002_03__y.csv',\n",
              "  'subject_003_02__x_time.csv',\n",
              "  'subject_003_02__x.csv',\n",
              "  'subject_003_02__y_time.csv',\n",
              "  'subject_003_02__y.csv',\n",
              "  'subject_005_02__x_time.csv',\n",
              "  'subject_005_02__x.csv',\n",
              "  'subject_005_02__y_time.csv',\n",
              "  'subject_005_02__y.csv',\n",
              "  'subject_005_03__x_time.csv',\n",
              "  'subject_005_03__x.csv',\n",
              "  'subject_005_03__y_time.csv',\n",
              "  'subject_005_03__y.csv'])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(training_files), training_files, validation_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ragzqERiKQw"
      },
      "source": [
        "# Dataset generation and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4bCibZV70NeW"
      },
      "outputs": [],
      "source": [
        "def windows(d, w, t):\n",
        "  r = np.arange(len(d))\n",
        "  s = r[::t]\n",
        "  z = list(zip(s, s + w))\n",
        "  f = '{0[0]}:{0[1]}'.format\n",
        "  g = lambda t: d.iloc[t[0]:t[1]]\n",
        "  ranges = list(map(f,z))\n",
        "  return ranges, pd.concat(map(g, z), keys=map(f, z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EziF5duX0Q3Q"
      },
      "outputs": [],
      "source": [
        "# def make_dataset(files, X, Y, freq_down=True, flag = False):\n",
        "\n",
        "#     overlap = 30\n",
        "#     if flag:\n",
        "#         overlap = 60\n",
        "#     total_length = 0\n",
        "#     for i in range(0, len(files), 4):\n",
        "#         x_time, x, y_time, y = files[i: i + 4]\n",
        "#         train_df = None\n",
        "#         if freq_down:\n",
        "#             #X_features\n",
        "#             x_time_df = pd.read_csv(TRAIN_PATH + x_time , header=None)\n",
        "#             x_df = pd.read_csv(TRAIN_PATH + x , header=None)\n",
        "#             x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
        "#             x_combined = x_combined.loc[range(1,len(x_combined), 4)].reset_index()  # down sampled the frequency\n",
        "\n",
        "#             #Y_labels\n",
        "#             y_time_df = pd.read_csv(TRAIN_PATH + y_time , header=None)\n",
        "#             y_df = pd.read_csv(TRAIN_PATH + y , header=None)\n",
        "#             y_combined = pd.concat([y_time_df, y_df], axis=1, ignore_index=True)\n",
        "\n",
        "#             train_df = pd.concat([x_combined, y_combined], axis=1, ignore_index=True)\n",
        "#             train_df = train_df.drop(columns=[0, 1, 8])  # Dropping the time stamp\n",
        "#             total_length += train_df.shape[0]\n",
        "#             ranges, windows_df = windows(train_df, 60, overlap)\n",
        "#             for ran in ranges:\n",
        "#                 l,r = ran.split(':')\n",
        "#                 df_range = windows_df.iloc[int(l): int(r)]\n",
        "#                 if int(r) > len(windows_df):\n",
        "#                     break\n",
        "#                 y_values = df_range[9].values\n",
        "#                 x_values = df_range.drop(columns=[9]).values\n",
        "#                 X.append(x_values)\n",
        "#                 Y.append(Counter(list(y_values)).most_common(1)[0][0])\n",
        "#         else:\n",
        "#             #X_features\n",
        "#             x_time_df = pd.read_csv(TRAIN_PATH + x_time, header=None)\n",
        "#             x_time_df.astype('float64')\n",
        "#             x_df = pd.read_csv(TRAIN_PATH + x, header=None)\n",
        "#             x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
        "#             x_combined = x_combined.rename({0:'timestamp'}, axis='columns')\n",
        "#             x_combined.set_index('timestamp', inplace=True)\n",
        "\n",
        "#             #Y_labels\n",
        "#             y_time_df = pd.read_csv(TRAIN_PATH + y_time , header=None)\n",
        "#             y_time_df.astype('float64')\n",
        "#             y_df = pd.read_csv(TRAIN_PATH + y, header=None)\n",
        "#             y_combined = pd.concat([y_time_df, y_df], axis=1, ignore_index=True)\n",
        "#             y_combined = y_combined.rename({0:'timestamp'}, axis='columns')\n",
        "#             y_combined.set_index('timestamp', inplace=True)\n",
        "#             train_df = pd.merge_asof(left=x_combined, right=y_combined, left_index=True, right_index = True, direction='nearest')  # Merging using the nearest values\n",
        "#             total_length += train_df.shape[0]\n",
        "#             ranges, windows_df = windows(train_df, 60, overlap)\n",
        "#             for ran in ranges:\n",
        "#                 l,r = ran.split(':')\n",
        "#                 df_range = windows_df.iloc[int(l): int(r)]\n",
        "#                 if int(r) > len(windows_df):\n",
        "#                     break\n",
        "#                 y_values = df_range['1_y'].values\n",
        "#                 x_values = df_range.drop(columns=['1_y']).values\n",
        "#                 X.append(x_values)\n",
        "#                 Y.append(Counter(list(y_values)).most_common(1)[0][0])\n",
        "#     print(total_length, len(X), len(y))\n",
        "#     return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_dataset(files, X, Y,  freq_down=True, window_size=60, overlap = 30, is_test = False):\n",
        "\n",
        "    overlap = overlap\n",
        "    if is_test:\n",
        "        overlap = overlap * 2\n",
        "    total_length = 0\n",
        "    for i in range(0, len(files), 4):\n",
        "        x_time, x, y_time, y = files[i: i + 4]\n",
        "        train_df = None\n",
        "        if freq_down:\n",
        "            #X_features\n",
        "            x_time_df = pd.read_csv(PATH + x_time , header=None)\n",
        "            x_df = pd.read_csv(PATH + x , header=None)\n",
        "            x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
        "            x_combined = x_combined.loc[range(1,len(x_combined), 4)].reset_index()  # down sampled the frequency\n",
        "\n",
        "            #Y_labels\n",
        "            y_time_df = pd.read_csv(PATH + y_time , header=None)\n",
        "            y_df = pd.read_csv(PATH + y , header=None)\n",
        "            y_combined = pd.concat([y_time_df, y_df], axis=1, ignore_index=True)\n",
        "\n",
        "            train_df = pd.concat([x_combined, y_combined], axis=1, ignore_index=True)\n",
        "            train_df = train_df.drop(columns=[0, 1, 8])  # Dropping the time stamp\n",
        "            total_length += train_df.shape[0]\n",
        "            ranges, windows_df = windows(train_df, window_size, overlap)\n",
        "            for ran in ranges:\n",
        "                l,r = ran.split(':')\n",
        "                df_range = windows_df.iloc[int(l): int(r)]\n",
        "                if int(r) > len(windows_df):\n",
        "                    break\n",
        "                y_values = df_range[9].values\n",
        "                x_values = df_range.drop(columns=[9]).values\n",
        "                X.append(x_values)\n",
        "                Y.append(Counter(list(y_values)).most_common(1)[0][0])\n",
        "        else:\n",
        "            #X_features\n",
        "            x_time_df = pd.read_csv(PATH + x_time, header=None)\n",
        "            x_time_df.astype('float64')\n",
        "            x_df = pd.read_csv(PATH + x, header=None)\n",
        "            x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
        "            x_combined = x_combined.rename({0:'timestamp'}, axis='columns')\n",
        "            x_combined.set_index('timestamp', inplace=True)\n",
        "\n",
        "            #Y_labels\n",
        "            y_time_df = pd.read_csv(PATH + y_time , header=None)\n",
        "            y_time_df.astype('float64')\n",
        "            y_df = pd.read_csv(PATH + y, header=None)\n",
        "            y_combined = pd.concat([y_time_df, y_df], axis=1, ignore_index=True)\n",
        "            y_combined = y_combined.rename({0:'timestamp'}, axis='columns')\n",
        "            y_combined.set_index('timestamp', inplace=True)\n",
        "            train_df = pd.merge_asof(left=x_combined, right=y_combined, left_index=True, right_index = True, direction='nearest')  # Merging using the nearest values\n",
        "            total_length += train_df.shape[0]\n",
        "            ranges, windows_df = windows(train_df, window_size, overlap)\n",
        "            for ran in ranges:\n",
        "                l,r = ran.split(':')\n",
        "                df_range = windows_df.iloc[int(l): int(r)]\n",
        "                if int(r) > len(windows_df):\n",
        "                    break\n",
        "                y_values = df_range['1_y'].values\n",
        "                x_values = df_range.drop(columns=['1_y']).values\n",
        "                X.append(x_values)\n",
        "                Y.append(Counter(list(y_values)).most_common(1)[0][0])\n",
        "    print(total_length, len(X), len(Y))\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def make_dataset_with_norm(files, X, Y, freq_down=True, window_size=60, overlap = 30, is_test = False, per_subject_norm=True, total_y = []):\n",
        "\n",
        "#     total_length = 0\n",
        "#     subject_files = defaultdict(list)\n",
        "    \n",
        "#     for i in range(1,9):\n",
        "#       temp_files = list(filter(lambda x: int(x.split('_')[1]) == i, files))\n",
        "#       subject_files[i].extend(temp_files)  # separating out all the files\n",
        "    \n",
        "#     for key, value in subject_files.items():\n",
        "#       cur_sub_files = value\n",
        "#       total_train_df = pd.DataFrame()\n",
        "#       if len(cur_sub_files) == 0: # skip if no files\n",
        "#         continue\n",
        "      \n",
        "#       for i in range(0, len(cur_sub_files), 4):\n",
        "#           x_time, x, y_time, y = cur_sub_files[i: i + 4]\n",
        "#           train_df = None\n",
        "#           if freq_down:\n",
        "#               #X_features\n",
        "#               x_time_df = pd.read_csv(TRAIN_PATH + x_time , header=None)\n",
        "#               x_df = pd.read_csv(TRAIN_PATH + x , header=None)\n",
        "#               x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
        "#               x_combined = x_combined.loc[range(1,len(x_combined), 4)].reset_index()  # Down sampled the frequency\n",
        "\n",
        "#               #Y_labels\n",
        "#               y_time_df = pd.read_csv(TRAIN_PATH + y_time , header=None)\n",
        "#               y_df = pd.read_csv(TRAIN_PATH + y , header=None)\n",
        "#               y_combined = pd.concat([y_time_df, y_df], axis=1, ignore_index=True)\n",
        "\n",
        "#               train_df = pd.concat([x_combined, y_combined], axis=1, ignore_index=True)\n",
        "#               train_df = train_df.drop(columns=[0, 1, 8])  # Dropping the time stamp\n",
        "#               train_df = train_df.rename({2:'acc_x',3:'acc_y',4:'acc_z',5:'gy_x',6:'gy_y',7:'gy_z',9:'target'}, axis='columns')\n",
        "#               total_length += train_df.shape[0]\n",
        "#               if is_test:\n",
        "#                 total_y.extend(list(y_df.values))\n",
        "#               if total_train_df.empty:\n",
        "#                 total_train_df = train_df\n",
        "#               else:\n",
        "#                 total_train_df = pd.concat([total_train_df, train_df], ignore_index = True)\n",
        "\n",
        "#           else:\n",
        "#               #X_features\n",
        "#               x_time_df = pd.read_csv(TRAIN_PATH + x_time, header=None)\n",
        "#               x_time_df.astype('float64')\n",
        "#               x_df = pd.read_csv(TRAIN_PATH + x, header=None)\n",
        "#               x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
        "#               x_combined = x_combined.rename({0:'timestamp'}, axis='columns')\n",
        "#               x_combined.set_index('timestamp', inplace=True)\n",
        "\n",
        "#               #Y_labels\n",
        "#               y_time_df = pd.read_csv(TRAIN_PATH + y_time , header=None)\n",
        "#               y_time_df.astype('float64')\n",
        "#               y_df = pd.read_csv(TRAIN_PATH + y, header=None)\n",
        "#               y_combined = pd.concat([y_time_df, y_df], axis=1, ignore_index=True)\n",
        "#               y_combined = y_combined.rename({0:'timestamp'}, axis='columns')\n",
        "#               y_combined.set_index('timestamp', inplace=True)\n",
        "#               train_df = pd.merge_asof(left=x_combined, right=y_combined, left_index=True, right_index = True, direction='nearest')  # Merging using the nearest values\n",
        "#               train_df = train_df.rename({'1_x':'acc_x',2:'acc_y',3:'acc_z',4:'gy_x',5:'gy_y',6:'gy_z','1_y':'target'}, axis='columns')\n",
        "#               total_length += train_df.shape[0]\n",
        "\n",
        "#               if is_test:\n",
        "#                 total_y.extend(list(y_df.values))\n",
        "                \n",
        "#               if total_train_df.empty:\n",
        "#                 total_train_df = train_df\n",
        "#               else:\n",
        "#                 total_train_df = pd.concat([total_train_df, train_df], ignore_index = True)\n",
        "      \n",
        "\n",
        "#       if per_subject_norm:   # after collating subject records, apply standard scalar to normalize subject data\n",
        "#         labels = total_train_df['target']\n",
        "#         labels = pd.DataFrame({'target':labels.values})\n",
        "#         temp_df = total_train_df.drop(columns=['target'])\n",
        "#         scaled_features = StandardScaler().fit_transform(temp_df.values)\n",
        "#         scaled_features_df = pd.DataFrame(scaled_features, index=temp_df.index, columns=temp_df.columns)\n",
        "#         # print(key, scaled_features_df.shape, labels.shape)\n",
        "#         total_train_df = pd.concat([scaled_features_df.reset_index(drop=True),labels.reset_index(drop=True)], axis=1, ignore_index=True)\n",
        "#         # print(key, total_train_df.shape)\n",
        "#         total_train_df.columns = ['acc_x','acc_y','acc_z','gy_x','gy_y','gy_z','target']\n",
        "     \n",
        "#       # print(total_train_df.describe())\n",
        "      \n",
        "#       ranges, windows_df = windows(total_train_df, window_size, overlap)\n",
        "#       for ran in ranges:\n",
        "#           l,r = ran.split(':')\n",
        "#           df_range = windows_df.iloc[int(l): int(r)]\n",
        "#           if int(r) > len(windows_df):\n",
        "#               break\n",
        "#           y_values = df_range['target'].values\n",
        "#           x_values = df_range.drop(columns=['target']).values\n",
        "\n",
        "#           X.append(x_values)\n",
        "          \n",
        "#           Y.append(Counter(list(y_values)).most_common(1)[0][0])\n",
        "      \n",
        "#     print(total_length, len(X), len(Y), len(total_y))\n",
        "#     return X, Y, total_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_dataset_with_norm(files, X, Y, freq_down=True, window_size=60, overlap = 30, is_test = False, per_subject_norm=True, total_y = [], per_session_norm=False):\n",
        "\n",
        "    total_length = 0\n",
        "    subject_files = defaultdict(list)\n",
        "    \n",
        "    for i in range(1,9):\n",
        "      temp_files = list(filter(lambda x: int(x.split('_')[1]) == i, files))\n",
        "      subject_files[i].extend(temp_files)  # separating out all the files\n",
        "    \n",
        "    for key, value in subject_files.items():\n",
        "      cur_sub_files = value\n",
        "      total_train_df = pd.DataFrame()\n",
        "      if len(cur_sub_files) == 0: # skip if no files\n",
        "        continue\n",
        "      \n",
        "      for i in range(0, len(cur_sub_files), 4):\n",
        "          x_time, x, y_time, y = cur_sub_files[i: i + 4]\n",
        "          train_df = None\n",
        "          if freq_down:\n",
        "              #X_features\n",
        "              x_time_df = pd.read_csv(PATH + x_time , header=None)\n",
        "              x_df = pd.read_csv(PATH + x , header=None)\n",
        "              x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
        "              x_combined = x_combined.loc[range(1,len(x_combined), 4)].reset_index()  # Down sampled the frequency\n",
        "\n",
        "              #Y_labels\n",
        "              y_time_df = pd.read_csv(PATH + y_time , header=None)\n",
        "              y_df = pd.read_csv(PATH + y , header=None)\n",
        "              y_combined = pd.concat([y_time_df, y_df], axis=1, ignore_index=True)\n",
        "\n",
        "              train_df = pd.concat([x_combined, y_combined], axis=1, ignore_index=True)\n",
        "              train_df = train_df.drop(columns=[0, 1, 8])  # Dropping the time stamp\n",
        "              train_df = train_df.rename({2:'acc_x',3:'acc_y',4:'acc_z',5:'gy_x',6:'gy_y',7:'gy_z',9:'target'}, axis='columns')\n",
        "              total_length += train_df.shape[0]\n",
        "              if is_test:\n",
        "                total_y.extend(list(y_df.values))\n",
        "              if total_train_df.empty:\n",
        "                total_train_df = train_df\n",
        "              else:\n",
        "                total_train_df = pd.concat([total_train_df, train_df], ignore_index = True)\n",
        "\n",
        "          else:\n",
        "              #X_features\n",
        "              x_time_df = pd.read_csv(PATH + x_time, header=None)\n",
        "              x_time_df.astype('float64')\n",
        "              x_df = pd.read_csv(PATH + x, header=None)\n",
        "              x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
        "              x_combined = x_combined.rename({0:'timestamp'}, axis='columns')\n",
        "              x_combined.set_index('timestamp', inplace=True)\n",
        "\n",
        "              #Y_labels\n",
        "              y_time_df = pd.read_csv(PATH + y_time , header=None)\n",
        "              y_time_df.astype('float64')\n",
        "              y_df = pd.read_csv(PATH + y, header=None)\n",
        "              y_combined = pd.concat([y_time_df, y_df], axis=1, ignore_index=True)\n",
        "              y_combined = y_combined.rename({0:'timestamp'}, axis='columns')\n",
        "              y_combined.set_index('timestamp', inplace=True)\n",
        "              train_df = pd.merge_asof(left=x_combined, right=y_combined, left_index=True, right_index = True, direction='nearest')  # Merging using the nearest values\n",
        "              train_df = train_df.rename({'1_x':'acc_x',2:'acc_y',3:'acc_z',4:'gy_x',5:'gy_y',6:'gy_z','1_y':'target'}, axis='columns')\n",
        "              total_length += train_df.shape[0]\n",
        "\n",
        "              if is_test:\n",
        "                total_y.extend(list(train_df['target'].values))\n",
        "\n",
        "              if per_session_norm:   # apply standard scalar to normalize session data\n",
        "                labels = train_df['target']\n",
        "                labels = pd.DataFrame({'target':labels.values})\n",
        "                temp_df = train_df.drop(columns=['target'])\n",
        "                scaled_features = StandardScaler().fit_transform(temp_df.values)\n",
        "                scaled_features_df = pd.DataFrame(scaled_features, index=temp_df.index, columns=temp_df.columns)\n",
        "                # print(key, scaled_features_df.shape, labels.shape)\n",
        "                train_df = pd.concat([scaled_features_df.reset_index(drop=True),labels.reset_index(drop=True)], axis=1, ignore_index=True)\n",
        "                # print(key, total_train_df.shape)\n",
        "                train_df.columns = ['acc_x','acc_y','acc_z','gy_x','gy_y','gy_z','target']\n",
        "                \n",
        "              if total_train_df.empty:\n",
        "                total_train_df = train_df\n",
        "              else:\n",
        "                total_train_df = pd.concat([total_train_df, train_df], ignore_index = True)\n",
        "      \n",
        "\n",
        "      if per_subject_norm:   # after collating subject records, apply standard scalar to normalize subject data\n",
        "        labels = total_train_df['target']\n",
        "        labels = pd.DataFrame({'target':labels.values})\n",
        "        temp_df = total_train_df.drop(columns=['target'])\n",
        "        scaled_features = StandardScaler().fit_transform(temp_df.values)\n",
        "        scaled_features_df = pd.DataFrame(scaled_features, index=temp_df.index, columns=temp_df.columns)\n",
        "        # print(key, scaled_features_df.shape, labels.shape)\n",
        "        total_train_df = pd.concat([scaled_features_df.reset_index(drop=True),labels.reset_index(drop=True)], axis=1, ignore_index=True)\n",
        "        # print(key, total_train_df.shape)\n",
        "        total_train_df.columns = ['acc_x','acc_y','acc_z','gy_x','gy_y','gy_z','target']\n",
        "     \n",
        "      # print(total_train_df.describe())\n",
        "      \n",
        "      # ranges, windows_df = windows(total_train_df, window_size, overlap)\n",
        "      # for ran in ranges:\n",
        "      #     l,r = ran.split(':')\n",
        "      #     df_range = windows_df.iloc[int(l): int(r)]\n",
        "      #     if int(r) > len(windows_df):\n",
        "      #         break\n",
        "      #     y_values = df_range['target'].values\n",
        "      #     x_values = df_range.drop(columns=['target']).values\n",
        "\n",
        "      #     X.append(x_values)\n",
        "          \n",
        "      #     Y.append(Counter(list(y_values)).most_common(1)[0][0])\n",
        "      addl_rows_df = pd.DataFrame(total_train_df[-window_size:])\n",
        "      total_train_df =  pd.concat([total_train_df, addl_rows_df], ignore_index = True)\n",
        "      for i in range(len(total_train_df) - window_size):\n",
        "          df_range = total_train_df.iloc[i:i+window_size]\n",
        "          \n",
        "          y_values = df_range['target'].values\n",
        "          x_values = df_range.drop(columns=['target']).values\n",
        "\n",
        "          X.append(x_values)\n",
        "          \n",
        "          Y.append(Counter(list(y_values)).most_common(1)[0][0])\n",
        "      \n",
        "    print(total_length, len(X), len(Y), len(total_y))\n",
        "    return X, Y, total_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiuGvC7PCVIw"
      },
      "source": [
        "freq_down boolean flag can be used to flip between 10hz and 40 hz frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "k6KJx9gq0SW8"
      },
      "outputs": [],
      "source": [
        "# train_X, train_y = [], []\n",
        "# valid_X, valid_y = [], []\n",
        "# test_X, test_y = [], []\n",
        "\n",
        "# make_dataset(files_train, train_X, train_y, freq_down=False, flag=True)\n",
        "# train_X, train_y = np.array(train_X, dtype=\"float32\"), np.array(train_y,dtype=\"float32\")\n",
        "# make_dataset(files_val, valid_X, valid_y, freq_down=False, flag=True)\n",
        "# valid_X, valid_y = np.array(valid_X, dtype=\"float32\"),np.array(valid_y, dtype=\"float32\")\n",
        "# # make_dataset(files_train, test_X, test_y, freq_down=True, flag=True)\n",
        "# # test_X, test_y = np.array(test_X, dtype=\"float32\"), np.array(test_y, dtype=\"float32\")\n",
        "# print(\"Done splitting the data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_X, train_y = [], []\n",
        "# valid_X, valid_y = [], []\n",
        "# test_X, test_y, test_total_y_down = [], [], []\n",
        "\n",
        "# norm_subjectwise = False\n",
        "# make_dataset(training_files, train_X, train_y, freq_down=False, flag=True)\n",
        "# # make_dataset_with_norm(training_files, train_X, train_y, freq_down=False, window_size=60, overlap=60, is_test=False, per_subject_norm=norm_subjectwise)\n",
        "# train_X, train_y = np.array(train_X, dtype=\"float32\"), np.array(train_y,dtype=\"float32\")\n",
        "# make_dataset(validation_files, valid_X, valid_y, freq_down=False, flag=True)\n",
        "# # make_dataset_with_norm(validation_files, valid_X, valid_y, freq_down=False, window_size=60, overlap=60, is_test=False, per_subject_norm=norm_subjectwise)\n",
        "# valid_X, valid_y = np.array(valid_X, dtype=\"float32\"),np.array(valid_y, dtype=\"float32\")\n",
        "# # make_dataset(files_test, test_X, test_y, freq_down=False, flag=True)\n",
        "# make_dataset_with_norm(files_test, test_X, test_y, freq_down=False, window_size=60, overlap=60, is_test=True, per_subject_norm=norm_subjectwise, total_y=test_total_y_down)\n",
        "# test_X, test_y, test_total_y_down = np.array(test_X, dtype=\"float32\"), np.array(test_y, dtype=\"float32\"),np.array(test_total_y_down, dtype=\"float32\") # test_total_y is actual 10hz sample with no windows\n",
        "# print(\"Done splitting the data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "915214 915214 915214 0\n",
            "273616 273616 273616 0\n",
            "152816 152816 152816 152816\n",
            "38204 38204 38204 38204\n",
            "Done splitting the data\n"
          ]
        }
      ],
      "source": [
        "train_X, train_y = [], []\n",
        "valid_X, valid_y = [], []\n",
        "test_X, test_y, test_total_y = [], [], []\n",
        "window_size = 30\n",
        "make_dataset_with_norm(training_files, train_X, train_y, freq_down=False, window_size=window_size, overlap=1, is_test=False, per_subject_norm=False,total_y=[],per_session_norm=True)\n",
        "train_X, train_y = np.array(train_X, dtype=\"float32\"), np.array(train_y,dtype=\"float32\")\n",
        "make_dataset_with_norm(validation_files, valid_X, valid_y, freq_down=False, window_size=window_size, overlap=1,is_test=False, per_subject_norm=False,total_y=[],per_session_norm=True)\n",
        "valid_X, valid_y = np.array(valid_X, dtype=\"float32\"),np.array(valid_y, dtype=\"float32\")\n",
        "make_dataset_with_norm(files_test, test_X, test_y, freq_down=False, window_size=window_size, overlap=1, is_test=True, per_subject_norm=False, total_y=test_total_y,per_session_norm=True)\n",
        "test_X, test_y, test_total_y = np.array(test_X, dtype=\"float32\"), np.array(test_y, dtype=\"float32\"),np.array(test_total_y, dtype=\"float32\")  # test_total_y is actual 40hz sample with no windows\n",
        "test_X_down, test_y_down, test_total_y_down = [], [], []\n",
        "make_dataset_with_norm(files_test, test_X_down, test_y_down, freq_down=True, window_size=window_size, overlap=1, is_test=True, per_subject_norm=False, total_y=test_total_y_down,per_session_norm=True)\n",
        "test_X_down, test_y_down, test_total_y_down = np.array([], dtype=\"float32\"), np.array([], dtype=\"float32\"),np.array(test_total_y_down, dtype=\"float32\") # test_total_y is actual 10hz sample with no windows\n",
        "print(\"Done splitting the data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "a-sGk6k3Ra8K"
      },
      "outputs": [],
      "source": [
        "# scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WvMybtGlRb_W"
      },
      "outputs": [],
      "source": [
        "# train_X = scaler.fit_transform(train_X.reshape(-1, train_X.shape[-1])).reshape(train_X.shape)\n",
        "# valid_X = scaler.transform(valid_X.reshape(-1, valid_X.shape[-1])).reshape(valid_X.shape)\n",
        "# test_X = scaler.transform(test_X.reshape(-1, test_X.shape[-1])).reshape(test_X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaVbLadQKKDU",
        "outputId": "d33f8844-e1b0-4c45-9200-2f1b32736dea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(915214, 30, 6)\n"
          ]
        }
      ],
      "source": [
        "train_X = np.array(train_X)\n",
        "print(train_X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOq31qC2K6ru",
        "outputId": "8593d9f9-d510-4646-f57f-f0577ef6f6a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "915214"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCEFqe20o4FP",
        "outputId": "b338a841-2167-43c8-b768-95ccac883e49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((152816, 30, 6), (273616, 30, 6), (152816,), (273616,))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_X.shape, valid_X.shape, test_y.shape, valid_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "swuH4KUQ0UAt"
      },
      "outputs": [],
      "source": [
        "weight  = compute_class_weight(class_weight = 'balanced', classes = np.unique(train_y), y = np.array(train_y))\n",
        "sample_weights = [round(weight[int(i)],2) for i in train_y]\n",
        "weight = torch.tensor(weight)\n",
        "weight = weight.cuda()\n",
        "weight = weight.to(torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqgujweVCk8G",
        "outputId": "f645ead1-c907-483f-e624-3aaa6fb3d442"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0., 1., 2., 3.], dtype=float32),\n",
              " tensor([0.3341, 5.6071, 4.1853, 1.6968], device='cuda:0'))"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(train_y), weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "M0oWXefe0wXJ"
      },
      "outputs": [],
      "source": [
        "# class Net(nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(Net, self).__init__()\n",
        "#     self.conv1 = nn.Conv1d(6,16,3)\n",
        "#     self.conv2 = nn.Conv1d(16,32,3)\n",
        "#     self.pool1 = nn.MaxPool1d(2,2)\n",
        "#     self.conv3 = nn.Conv1d(32,64,3)\n",
        "#     self.conv4 = nn.Conv1d(64,128,3)\n",
        "#     self.pool2 = nn.MaxPool1d(2,2)\n",
        "#     self.fc1 = nn.Linear(512, 120)\n",
        "#     self.fc2 = nn.Linear(120, 64)\n",
        "#     self.fc3 = nn.Linear(64,4)\n",
        "#     self.dropout1 = nn.Dropout(0.1)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x = F.relu(self.conv1(x.permute(0,2,1)))\n",
        "#     x = self.pool1(F.relu(self.dropout1(self.conv2(x))))\n",
        "#     x = F.relu(self.conv3(x))\n",
        "#     x = self.pool2(F.relu(self.dropout1(self.conv4(x))))\n",
        "#     # print(\"1\",x.shape)\n",
        "#     # x = x.view(-1,3456)\n",
        "#     x = torch.flatten(x,1)\n",
        "#     # print(\"2\",x.shape)\n",
        "#     x = F.relu(self.dropout1(self.fc1(x)))\n",
        "#     x = F.relu(self.dropout1(self.fc2(x)))\n",
        "#     # print(\"3\", x.shape)\n",
        "#     x = self.fc3(x)\n",
        "#     return x\n",
        "\n",
        "# model = Net()\n",
        "# if flag_cuda:\n",
        "#   model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "nrhaSc0SmXbN"
      },
      "outputs": [],
      "source": [
        "# class Net2(nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(Net2, self).__init__()\n",
        "#     self.conv1 = nn.Conv1d(6,16,3)\n",
        "#     self.conv2 = nn.Conv1d(16,32,3)\n",
        "#     self.pool1 = nn.AvgPool1d(2,2)\n",
        "#     self.conv3 = nn.Conv1d(32,64,3)\n",
        "#     self.conv4 = nn.Conv1d(64,128,3)\n",
        "#     self.pool2 = nn.MaxPool1d(2,2)\n",
        "#     self.fc1 = nn.Linear(1536, 120)\n",
        "#     self.fc2 = nn.Linear(120, 64)\n",
        "#     self.fc3 = nn.Linear(64,4)\n",
        "#     self.dropout1 = nn.Dropout(0.1)\n",
        "#     self.dropout2 = nn.Dropout(0.25)\n",
        "#     # self.batch_norm = nn.BatchNorm1d()\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x = F.relu(self.conv1(x.permute(0,2,1)))\n",
        "#     x = self.pool1(self.dropout2(F.relu(self.conv2(x))))\n",
        "#     # print(\"1\",x.shape)\n",
        "#     x = F.relu(self.conv3(x))\n",
        "#     x = self.pool2(self.dropout1(F.relu(self.conv4(x))))\n",
        "#     x = torch.flatten(x,1)\n",
        "#     # print(\"2\",x.shape)\n",
        "#     x = self.dropout1(F.relu(self.fc1(x)))\n",
        "#     # print(\"3\", x.shape)\n",
        "#     x = self.dropout1(F.relu(self.fc2(x)))\n",
        "#     x = self.fc3(x)\n",
        "#     return x\n",
        "\n",
        "# model = Net2()\n",
        "# if flag_cuda:\n",
        "#   model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sze2ETbtKU1v"
      },
      "outputs": [],
      "source": [
        "# class BiLSTM(nn.Module):\n",
        "#   def __init__(self, window_size=30):\n",
        "#     super(BiLSTM, self).__init__()\n",
        "#     self.lstm1 = nn.LSTM(6, window_size, 2,batch_first=True, bidirectional=True)\n",
        "#     self.lstm2 =  nn.LSTM(window_size * 2, window_size, 2,batch_first=True,bidirectional=True)\n",
        "#     self.lstm3 =  nn.LSTM(window_size * 2, window_size, 2, batch_first=True,bidirectional=True)\n",
        "#     self.fc1 = nn.Linear(window_size * 2 * window_size, window_size * 2)\n",
        "#     self.fc2 = nn.Linear(window_size * 2, 64)\n",
        "#     self.fc3 = nn.Linear(64, 4)\n",
        "#     self.dropout1 = nn.Dropout(0.1)\n",
        "#     self.dropout2 = nn.Dropout(0.25)\n",
        "#     self.dropout3 = nn.Dropout(0.2)\n",
        "#     self.batch_norm = nn.BatchNorm1d(window_size * 2)\n",
        "\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x, self.hidden1 = self.lstm1(x)\n",
        "#     x = self.dropout3(self.batch_norm(F.relu(x).permute(0, 2, 1)))\n",
        "#     x = self.dropout1(self.batch_norm(F.relu(self.lstm2(x.permute(0, 2, 1))[0]).permute(0,2,1)))\n",
        "#     # print(\"1\", x.shape)\n",
        "#     x = self.dropout3(F.relu(self.lstm3(x.permute(0, 2, 1))[0]))\n",
        "#     # print(\"3\", x.shape)\n",
        "#     x = torch.flatten(x,1)\n",
        "#     x = self.dropout1(F.relu(self.fc1(x)))\n",
        "#     # print(\"3\", x.shape)\n",
        "#     x = self.dropout1(F.relu(self.fc2(x)))\n",
        "#     x = self.fc3(x)\n",
        "#     # x = nn.Softmax(dim=-1)(x)\n",
        "#     return x\n",
        "\n",
        "# model = BiLSTM()\n",
        "# if flag_cuda:\n",
        "#   model.cuda()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        \n",
        "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "model = SimpleLSTM(6, 120, 1, 4)\n",
        "if flag_cuda:\n",
        "    model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "ehDfKGKhn684"
      },
      "outputs": [],
      "source": [
        "# class ConvLSTM(nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(ConvLSTM, self).__init__()\n",
        "#     self.conv1 = nn.Conv1d(6,16,3)\n",
        "#     self.lstm1 = nn.LSTM(16,60,2,batch_first=True, bidirectional=True)\n",
        "#     self.lstm2 =  nn.LSTM(120,60,2,batch_first=True,bidirectional=True)\n",
        "#     self.lstm3 =  nn.LSTM(120,60,2,batch_first=True,bidirectional=True)\n",
        "#     self.fc1 = nn.Linear(7200, 120)\n",
        "#     self.fc2 = nn.Linear(120, 64)\n",
        "#     self.fc3 = nn.Linear(64,4)\n",
        "#     self.dropout1 = nn.Dropout(0.1)\n",
        "#     self.dropout2 = nn.Dropout(0.25)\n",
        "#     self.dropout3 = nn.Dropout(0.2)\n",
        "#     self.batch_norm = nn.BatchNorm1d(120)\n",
        "\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x, self.hidden1 = self.lstm1(x)\n",
        "#     x = self.dropout3(self.batch_norm(F.relu(x).permute(0, 2, 1)))\n",
        "#     x = self.dropout1(self.batch_norm(F.relu(self.lstm2(x.permute(0, 2, 1))[0]).permute(0,2,1)))\n",
        "#     # print(\"1\", x.shape)\n",
        "#     x = self.dropout3(F.relu(self.lstm3(x.permute(0, 2, 1))[0]))\n",
        "#     # print(\"3\", x.shape)\n",
        "#     x = torch.flatten(x,1)\n",
        "#     x = self.dropout1(F.relu(self.fc1(x)))\n",
        "#     # print(\"3\", x.shape)\n",
        "#     x = self.dropout1(F.relu(self.fc2(x)))\n",
        "#     x = self.fc3(x)\n",
        "#     return x\n",
        "\n",
        "# model = BiLSTM()\n",
        "# if flag_cuda:\n",
        "#   model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=None, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        ce_loss = nn.CrossEntropyLoss(weight=self.alpha)(input, target)  # Cross-entropy loss\n",
        "        pt = torch.exp(-ce_loss)  # Probability of true class\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss  # Focal loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "nnme1uCP08vv"
      },
      "outputs": [],
      "source": [
        "# Specifying the loss function\n",
        "criterion = nn.CrossEntropyLoss(weight = weight.float())\n",
        "# criterion = FocalLoss(gamma=2, alpha=weight)\n",
        "\n",
        "# Specify optimizer\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "tKBiB_qQ_Zj6",
        "outputId": "54a0a006-1fa3-4d34-c840-84624c6dc001"
      },
      "outputs": [],
      "source": [
        "# plt.style.use('seaborn')\n",
        "# plt.figure(figsize=(8,6),dpi=100)\n",
        "# text = [\"standing / walking\", \"going downstairs\",\"going upstairs\", \"walking on grass\"]\n",
        "# labels = list(dict(Counter(train_y)).values())\n",
        "# for bar in range(0,4):\n",
        "#     plt.bar(text[bar],labels[bar])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "CEWQrkRT_fLj",
        "outputId": "0663ec9e-dd8b-44c5-a283-e0747eff3863"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(8,6),dpi=100)\n",
        "# plt.style.use('seaborn')\n",
        "# text = [\"standing / walking\", \"going downstairs\",\"going upstairs\", \"walking on grass\"]\n",
        "# labels = list(dict(Counter(valid_y)).values())\n",
        "# for bar in range(0,4):\n",
        "#     plt.bar(text[bar],labels[bar])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "jTRdlKpXThbU"
      },
      "outputs": [],
      "source": [
        "# weighted_sampler = WeightedRandomSampler(\n",
        "#     weights=sample_weights,\n",
        "#     num_samples=len(sample_weights),\n",
        "#     replacement=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "54dQAPqvbYOW"
      },
      "outputs": [],
      "source": [
        "class TerrainDataset(Dataset):\n",
        "  def __init__(self, data, target):\n",
        "    self.data = np.array(data)\n",
        "    self.target = np.array(target)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.data[index], self.target[index]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "qJgPMVFC3XA2"
      },
      "outputs": [],
      "source": [
        "batch_size = 120 #60 #120\n",
        "train_dataset = TerrainDataset(train_X, train_y)\n",
        "# x_train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = batch_size, sampler=weighted_sampler, shuffle=False)\n",
        "x_train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = batch_size, shuffle=True)\n",
        "# y_train_loader = torch.utils.data.DataLoader(train_y,batch_size = batch_size, sampler=weighted_sampler)\n",
        "validation_dataset = TerrainDataset(valid_X, valid_y)\n",
        "x_validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size = batch_size, shuffle=False)\n",
        "# y_validation_loader = torch.utils.data.DataLoader(valid_y, batch_size = batch_size)\n",
        "\n",
        "# test_dataset = TerrainDataset(test_X, test_y)\n",
        "test_dataset = TerrainDataset(test_X, test_y)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n",
        "# y_test_loader = torch.utils.data.DataLoader(test_y, batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8amzZ-TKjW5x",
        "outputId": "2cb7db66-a441-4c50-a795-2ffd2fe9a78e"
      },
      "outputs": [],
      "source": [
        "# for data, target in x_train_loader:\n",
        "#   print(list(map(int, list(target))))\n",
        "#   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1bXsGOy23aTp",
        "outputId": "e938c43d-625d-41b4-e3f0-ad2c5c41a49d"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[198], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m   \u001b[39mreturn\u001b[39;00m train_losslist, valid_losslist\n\u001b[0;32m     68\u001b[0m \u001b[39m# Executing the training\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m train_losslist, valid_losslist \u001b[39m=\u001b[39m trainNet(model,criterion,optimizer,n_epochs,flag_cuda)\n\u001b[0;32m     71\u001b[0m \u001b[39m# Loading the best model\u001b[39;00m\n\u001b[0;32m     72\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mmodel.pt\u001b[39m\u001b[39m'\u001b[39m))\n",
            "Cell \u001b[1;32mIn[198], line 17\u001b[0m, in \u001b[0;36mtrainNet\u001b[1;34m(model, criterion, optimizer, n_epochs, flag_cuda)\u001b[0m\n\u001b[0;32m     13\u001b[0m   valid_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[39m#   model.train()\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m   \u001b[39mfor\u001b[39;00m data,target \u001b[39min\u001b[39;00m x_train_loader:\n\u001b[0;32m     18\u001b[0m       \u001b[39mif\u001b[39;00m flag_cuda:\n\u001b[0;32m     19\u001b[0m           data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mcuda(), target\u001b[39m.\u001b[39mcuda()\n",
            "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\BRaTS2021\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\BRaTS2021\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\BRaTS2021\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
            "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\BRaTS2021\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    204\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
            "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\BRaTS2021\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\BRaTS2021\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\BRaTS2021\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
            "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\BRaTS2021\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m np_str_obj_array_pattern\u001b[39m.\u001b[39msearch(elem\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mstr) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem\u001b[39m.\u001b[39mdtype))\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m collate([torch\u001b[39m.\u001b[39;49mas_tensor(b) \u001b[39mfor\u001b[39;49;00m b \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "n_epochs = 10 #25\n",
        "\n",
        "def trainNet(model,criterion,optimizer,n_epochs,flag_cuda):\n",
        "\n",
        "  train_losslist = []\n",
        "  valid_losslist = []\n",
        "  valid_loss_min = np.Inf # track change in validation loss\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "      # Keeping track of training and validation loss\n",
        "      train_loss = 0.0\n",
        "      valid_loss = 0.0\n",
        "    \n",
        "    #   model.train()\n",
        "\n",
        "      for data,target in x_train_loader:\n",
        "          if flag_cuda:\n",
        "              data, target = data.cuda(), target.cuda()\n",
        "          optimizer.zero_grad()\n",
        "          output = model(data)\n",
        "          target = target.type(torch.LongTensor) \n",
        "          if flag_cuda:\n",
        "            output, target = output.cuda(), target.cuda()\n",
        "          loss = criterion(output, target)\n",
        "\n",
        "          # Backward pass: compute gradient of loss with respect to parameters\n",
        "          loss.backward()\n",
        "          # Perform a single optimization step (parameter update)\n",
        "          optimizer.step()\n",
        "          # Update training loss\n",
        "          train_loss += loss.item()*data.size(0)\n",
        "          \n",
        "    #   model.eval()\n",
        "\n",
        "      for data,target in x_validation_loader:\n",
        "          with torch.no_grad():\n",
        "            if flag_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            output = model(data)\n",
        "            target = target.type(torch.LongTensor) \n",
        "\n",
        "            if flag_cuda:\n",
        "                output, target = output.cuda(), target.cuda()\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "            valid_loss += loss.item()*data.size(0)\n",
        "        \n",
        "      # Calculating average losses\n",
        "      train_loss = train_loss/len(train_X)\n",
        "      valid_loss = valid_loss/len(valid_X)\n",
        "      train_losslist.append(train_loss)\n",
        "      valid_losslist.append(valid_loss)\n",
        "          \n",
        "      #Printing training/validation statistics \n",
        "      print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "          epoch, train_loss, valid_loss))\n",
        "      \n",
        "      # Saving model if validation loss has decreased\n",
        "      if valid_loss < valid_loss_min:\n",
        "          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "              valid_loss_min,valid_loss))\n",
        "          torch.save(model.state_dict(), 'model.pt')\n",
        "          valid_loss_min = valid_loss\n",
        "        \n",
        "  return train_losslist, valid_losslist\n",
        "\n",
        "# Executing the training\n",
        "train_losslist, valid_losslist = trainNet(model,criterion,optimizer,n_epochs,flag_cuda)\n",
        "\n",
        "# Loading the best model\n",
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "# Plotting the learning curves\n",
        "# plt.plot(range(1, n_epochs + 1), train_losslist, valid_losslist)\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.legend(['Training','Validation'])\n",
        "# plt.title(\"Performance of Baseline Model\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8vZ1Rlq3ofC",
        "outputId": "13b66507-4278-415a-eabf-e2c004b4a4cd"
      },
      "outputs": [],
      "source": [
        "classes = [0.,1.,2.,3.]\n",
        "y_prediction = []\n",
        "\n",
        "def assessNet(model,criterion, x_test_loader):\n",
        "  # Tracking test loss and accuracy\n",
        "  test_loss = 0.0\n",
        "  class_correct = list(0. for i in range(4))\n",
        "  class_total = list(0. for i in range(4))\n",
        "\n",
        "  # Setting model to evaluate\n",
        "  model.eval()\n",
        "\n",
        "  # Iterating over batches of test data\n",
        "\n",
        "#   print(valid_X.shape)\n",
        "\n",
        "  for data,target in x_test_loader:\n",
        "      # Obtaining predictions and loss\n",
        "      if flag_cuda:\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "      output = model(data)\n",
        "      # output = torch.argmax(output,dim = 1)\n",
        "      target = target.type(torch.LongTensor) \n",
        "      if flag_cuda:\n",
        "        output, target = output.cuda(), target.cuda()\n",
        "      loss = criterion(output, target)\n",
        "      test_loss += loss.item()*data.size(0)\n",
        "      _, pred = torch.max(output, 1)    \n",
        "      # Comparing predictions to true label\n",
        "      correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "      # print(pred)\n",
        "      y_prediction.extend(pred.cpu().numpy())\n",
        "      correct = np.squeeze(correct_tensor.numpy()) if not flag_cuda else np.squeeze(correct_tensor.cpu().numpy())\n",
        "      # Calculating test accuracy for each object class\n",
        "      for i in range(len(output)):\n",
        "          label = target.data[i]\n",
        "          class_correct[label] += correct[i].item()\n",
        "          class_total[label] += 1\n",
        "\n",
        "  # Computing the average test loss\n",
        "  test_loss = test_loss/len(valid_X)\n",
        "  print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "  # Computing the class accuracies\n",
        "  for i in range(4):\n",
        "      if class_total[i] > 0:\n",
        "          print('Test Accuracy of %10s: %2d%% (%2d/%2d)' % (\n",
        "              classes[i], 100 * class_correct[i] / class_total[i],\n",
        "              np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "      else:\n",
        "          print('Test Accuracy of %10s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "  # Computing the overall accuracy\n",
        "  print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "      100. * np.sum(class_correct) / np.sum(class_total),\n",
        "      np.sum(class_correct), np.sum(class_total)))\n",
        "\n",
        "\n",
        "  return\n",
        "\n",
        "# model.load_state_dict(torch.load(r'C:\\Users\\sumuk\\OneDrive\\Desktop\\NCSU_related\\Courses_and_stuff\\Courses_and_stuff\\NCSU_courses_and_books\\ECE_542\\CompetitiveProject\\Models\\BiLSTM_new_splits_2\\model_0.41.pt'))\n",
        "model.load_state_dict(torch.load(\"model.pt\"))\n",
        "model.eval()\n",
        "# assessNet(model,criterion, x_validation_loader)\n",
        "assessNet(model, criterion, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnAFoDL-1VaR",
        "outputId": "1dd894f7-d735-4bf1-bc17-bc017c91da36"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true=test_y, y_pred=y_prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "A0N-U21lFEWG",
        "outputId": "b66b5927-8738-4d64-8fd4-37e4e1ed4ff8"
      },
      "outputs": [],
      "source": [
        "# cm = confusion_matrix(y_true=test_y, y_pred=y_prediction)\n",
        "# cmd = ConfusionMatrixDisplay(cm, display_labels=[\"standing / walking\", \"going downstairs\",\"going upstairs\", \"walking on grass\"])\n",
        "# cmd.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "tr0cJKbCFjo_",
        "outputId": "6389be1d-46c4-4a04-8355-bafa42fd78e0"
      },
      "outputs": [],
      "source": [
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt     \n",
        "\n",
        "# ax= plt.subplot()\n",
        "# sns.heatmap(cm, annot=True, fmt='g', ax=ax)  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
        "\n",
        "# # labels, title and ticks\n",
        "# ax.set_xlabel('Predicted labels')\n",
        "# ax.set_ylabel('True labels') \n",
        "# ax.set_title('Confusion Matrix') \n",
        "# ax.xaxis.set_ticklabels([\"standing / walking\", \"going downstairs\",\"going upstairs\", \"walking on grass\"]) \n",
        "# ax.yaxis.set_ticklabels([\"standing / walking\", \"going downstairs\",\"going upstairs\", \"walking on grass\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_classification(y_pred, y_actual,window_size,down_sampled=True):\n",
        "  y_pred_total = []\n",
        "  for i in range(0,len(y_pred),4):\n",
        "      y_pred_total.extend([Counter(y_pred[i:i+4]).most_common(1)[0][0]])\n",
        "  # for i in y_pred:\n",
        "  #   # current_pred = [int(i)] * window_size\n",
        "  #   if down_sampled:\n",
        "  #     for i in range(0,len(current_pred),4):\n",
        "  #       y_pred_total.extend([Counter(current_pred[i:i+4]).most_common(1)[0][0]])\n",
        "  #   else:\n",
        "  #     y_pred_total.extend([i]*window_size)\n",
        "\n",
        "  print(len(y_pred_total), len(y_actual))\n",
        "  \n",
        "  len_pred = len(y_pred_total)\n",
        "  len_actual = len(y_actual)\n",
        "\n",
        "  # if len_actual > len_pred:\n",
        "  #   diff = len_actual - len_pred\n",
        "  #   y_pred_total.extend([y_pred_total[-1]]* diff)\n",
        "\n",
        "  # elif len_pred > len_actual:\n",
        "  #   y_pred_total = y_pred_total[:len_actual] \n",
        "  \n",
        "  print(len(y_pred_total), len(y_actual))\n",
        "\n",
        "  print(classification_report(y_true=y_actual, y_pred=y_pred_total))\n",
        "\n",
        "get_classification(y_prediction, test_total_y_down, window_size=60, down_sampled=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH5UcZtae3cB"
      },
      "source": [
        "# Sensor data subject wise analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2CYwi-JU5IW"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4tywSg7e1eC"
      },
      "outputs": [],
      "source": [
        "subject_stats = {'mean': [], 'min':[], 'max':[]}\n",
        "actual_stats = {}\n",
        "\n",
        "for i in range(6):  # 6 columns stats for all the subjects\n",
        "  actual_stats[i] = copy.deepcopy(subject_stats)\n",
        "\n",
        "for key, value in subject_files.items():\n",
        "  local_files = value\n",
        "  y_labels = []\n",
        "  train_df = None\n",
        "  for i in range(0, len(local_files), 4):\n",
        "          x_time, x, y_time, y = local_files[i: i + 4]\n",
        "          train_df = None\n",
        "        \n",
        "          #X_features\n",
        "          x_time_df = pd.read_csv(PATH + x_time, header=None)\n",
        "          x_time_df.astype('float64')\n",
        "          x_df = pd.read_csv(PATH + x, header=None)\n",
        "          x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
        "          x_combined = x_combined.rename({0:'timestamp'}, axis='columns')\n",
        "          x_combined.set_index('timestamp', inplace=True)\n",
        "\n",
        "          #Y_labels\n",
        "          y_time_df = pd.read_csv(PATH + y_time , header=None)\n",
        "          y_time_df.astype('float64')\n",
        "          y_df = pd.read_csv(PATH + y, header=None)\n",
        "          y_combined = pd.concat([y_time_df, y_df], axis=1, ignore_index=True)\n",
        "          y_combined = y_combined.rename({0:'timestamp'}, axis='columns')\n",
        "          y_combined.set_index('timestamp', inplace=True)\n",
        "          train_df = pd.merge_asof(left=x_combined, right=y_combined, left_index=True, right_index = True, direction='nearest')  # Merging using the nearest values\n",
        "          y_labels.extend(train_df['1_y'].values)\n",
        "          train_df = train_df.drop(columns=['1_y'])\n",
        "  \n",
        "  for index, col in enumerate(train_df.columns[:6]):\n",
        "    # print(train_df[col].mean())\n",
        "    actual_stats[index]['mean'].append(train_df[col].mean())\n",
        "    actual_stats[index]['min'].append(train_df[col].min())\n",
        "    actual_stats[index]['max'].append(train_df[col].max())\n",
        "\n",
        "  '''\n",
        "  print(f\"Subject {key} \\n\", train_df.columns[:6])     # Uncomment to plot the Class distribution for each subject\n",
        "  print(key, len(y_labels))\n",
        "  plt.style.use('seaborn')\n",
        "  plt.figure(figsize=(8,6),dpi=100)\n",
        "  plt.title(f\"Subject {key} class distribution\")\n",
        "  text = [\"standing / walking\", \"going downstairs\",\"going upstairs\", \"walking on grass\"]\n",
        "  labels = list(dict(Counter(y_labels)).values())\n",
        "  for bar in range(0,4):\n",
        "      plt.bar(text[bar],labels[bar])\n",
        "  '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fgjBKGlDfDwf",
        "outputId": "6c90100b-8671-431e-b0b0-ad66a71c5c0a"
      },
      "outputs": [],
      "source": [
        "for key, value in actual_stats.items():\n",
        "\n",
        "  plt.style.use('seaborn')\n",
        "  plt.figure(figsize=(8,6),dpi=100)\n",
        "  plt.title(f\"Column {key} Mean for each subject\")\n",
        "  text = [f'subject-{i}' for i in range(1,9)]\n",
        "  stats = value['mean']\n",
        "  for bar in range(8):\n",
        "      plt.bar(text[bar],stats[bar])\n",
        "\n",
        "  plt.style.use('seaborn')\n",
        "  plt.figure(figsize=(8,6),dpi=100)\n",
        "  plt.title(f\"Column {key} Min for each subject\")\n",
        "  stats = value['min']\n",
        "  for bar in range(8):\n",
        "      plt.bar(text[bar],stats[bar])\n",
        "\n",
        "\n",
        "  plt.style.use('seaborn')\n",
        "  plt.figure(figsize=(8,6),dpi=100)\n",
        "  plt.title(f\"Column {key} Max for each subject\")\n",
        "  stats = value['max']\n",
        "  for bar in range(8):\n",
        "      plt.bar(text[bar],stats[bar])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx8vaFTTh1Eg"
      },
      "source": [
        "# Others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVDVjj0dBfd1"
      },
      "outputs": [],
      "source": [
        "for key, value in subject_files.items():\n",
        "  local_files = value\n",
        "  y_labels = []\n",
        "  for i in range(0, len(local_files), 4):\n",
        "          x_time, x, y_time, y = local_files[i: i + 4]\n",
        "          train_df = None\n",
        "        \n",
        "          #X_features\n",
        "          x_time_df = pd.read_csv(PATH + x_time, header=None)\n",
        "          x_time_df.astype('float64')\n",
        "          x_df = pd.read_csv(PATH + x, header=None)\n",
        "          x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
        "          x_combined = x_combined.rename({0:'timestamp'}, axis='columns')\n",
        "          x_combined.set_index('timestamp', inplace=True)\n",
        "\n",
        "          #Y_labels\n",
        "          y_time_df = pd.read_csv(PATH + y_time , header=None)\n",
        "          y_time_df.astype('float64')\n",
        "          y_df = pd.read_csv(PATH + y, header=None)\n",
        "          y_combined = pd.concat([y_time_df, y_df], axis=1, ignore_index=True)\n",
        "          y_combined = y_combined.rename({0:'timestamp'}, axis='columns')\n",
        "          y_combined.set_index('timestamp', inplace=True)\n",
        "          train_df = pd.merge_asof(left=x_combined, right=y_combined, left_index=True, right_index = True, direction='nearest')  # Merging using the nearest values\n",
        "          y_labels.extend(train_df['1_y'].values)\n",
        "  print(key, len(y_labels))\n",
        "  plt.style.use('seaborn')\n",
        "  plt.figure(figsize=(8,6),dpi=100)\n",
        "  plt.title(f\"Subject {key} class distribution\")\n",
        "  text = [\"standing / walking\", \"going downstairs\",\"going upstairs\", \"walking on grass\"]\n",
        "  labels = list(dict(Counter(y_labels)).values())\n",
        "  for bar in range(0,4):\n",
        "      plt.bar(text[bar],labels[bar])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXwh7ubxD9Yi",
        "outputId": "a235a88b-4af0-4510-c9cc-39df607a855b"
      },
      "outputs": [],
      "source": [
        "subject_files[1][:10], subject_files[2][:10] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5VU_kLTBzUh"
      },
      "outputs": [],
      "source": [
        "subject_files = defaultdict(list)\n",
        "total_files = []\n",
        "for filename in os.listdir(PATH):\n",
        "    total_files.append(filename)\n",
        "total_files = sorted(total_files, key = lambda x: (int(x.split('_')[1]),int(x.split('_')[2]), x.split('_')[4] ))\n",
        "for i in range(1,9):\n",
        "  files = list(filter(lambda x: int(x.split('_')[1]) == i, total_files))\n",
        "  subject_files[i].extend(files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx3aFoYiCLok"
      },
      "source": [
        "Checking the class distribution in each subject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1EWf38l4SNs"
      },
      "outputs": [],
      "source": [
        "model = MGRU_FCN(6,4,25)\n",
        "model.cuda()\n",
        "# Specifying the loss function\n",
        "criterion = nn.CrossEntropyLoss(weight = weight.float())\n",
        "\n",
        "# Specify optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq5voBNFANo-"
      },
      "outputs": [],
      "source": [
        "# class Resnet(nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(Resnet, self).__init__()\n",
        "#     self.resnet = ResNet(6,4)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x = self.resnet(x.permute(0,2,1))\n",
        "#     return x\n",
        "\n",
        "model = LSTM_FCN(6,4,60)\n",
        "model.cuda()\n",
        "# Specifying the loss function\n",
        "criterion = nn.CrossEntropyLoss(weight = weight.float())\n",
        "\n",
        "# Specify optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx9asgeY2w2o"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.signal import find_peaks"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BiLSTM()\n",
        "if flag_cuda:\n",
        "  model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ex_x, ex_y = train_dataset[0]\n",
        "# train_dataset.__getitem__(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ex_x.shape, ex_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = model(torch.Tensor(ex_x[np.newaxis, :, :]).cuda())\n",
        "pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion(pred, torch.Tensor(ex_y).unsqueeze(0).cuda())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Discrepency checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "true_test_files = sorted(os.listdir(TEST_PATH))\n",
        "target_path = \"Trial_2/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TerrainDataset2(Dataset):\n",
        "  def __init__(self, data):\n",
        "    self.data = np.array(data)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.data[index]\n",
        "\n",
        "\n",
        "def get_pred(y_pred, len_actual, window_size=60):\n",
        "\n",
        "  y_pred_total = []\n",
        "  print(len(y_pred))\n",
        "  for i in y_pred:\n",
        "    current_pred = [int(i)] * window_size\n",
        "    \n",
        "    for i in range(0,len(current_pred),4):\n",
        "      y_pred_total.extend([Counter(current_pred[i:i+4]).most_common(1)[0][0]])\n",
        "\n",
        "  len_pred = len(y_pred_total)\n",
        "\n",
        "  if len_actual > len_pred:\n",
        "    diff = len_actual - len_pred\n",
        "    y_pred_total.extend([y_pred_total[-1]]* diff)\n",
        "  \n",
        "  return y_pred_total\n",
        "\n",
        "\n",
        "\n",
        "def make_dataset_with_norm(true_test_files, model1_path, model2_path, window_size=60, overlap = 30, per_subject_norm=True):\n",
        "  for i in range(0, len(true_test_files), 3):\n",
        "      X = []\n",
        "      x_time, x, y_time = true_test_files[i: i + 3]\n",
        "      train_df = None\n",
        "      total_train_df = pd.DataFrame()\n",
        "      \n",
        "\n",
        "      #X_features\n",
        "      x_time_df = pd.read_csv(TEST_PATH + x_time, header=None)\n",
        "      x_time_df.astype('float64')\n",
        "      x_df = pd.read_csv(TEST_PATH + x, header=None)\n",
        "      x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
        "      x_combined = x_combined.rename({0:'timestamp'}, axis='columns')\n",
        "      x_combined.set_index('timestamp', inplace=True)\n",
        "\n",
        "      #Y_labels\n",
        "      y_time_df = pd.read_csv(TEST_PATH + y_time , header=None)\n",
        "      y_time_df.astype('float64')\n",
        "      train_df = x_combined.rename({'1':'acc_x',2:'acc_y',3:'acc_z',4:'gy_x',5:'gy_y',6:'gy_z'}, axis='columns')\n",
        "\n",
        "        \n",
        "      if total_train_df.empty:\n",
        "        total_train_df = train_df\n",
        "      else:\n",
        "        total_train_df = pd.concat([total_train_df, train_df], ignore_index = True)\n",
        "        \n",
        "\n",
        "      if per_subject_norm:   # after collating subject records, apply standard scalar to normalize subject data\n",
        "        temp_df = total_train_df.copy()\n",
        "        scaled_features = StandardScaler().fit_transform(temp_df.values)\n",
        "        total_train_df = pd.DataFrame(scaled_features, index=temp_df.index, columns=temp_df.columns)\n",
        "        # print(key, scaled_features_df.shape, labels.shape)\n",
        "        # print(key, total_train_df.shape)\n",
        "        total_train_df.columns = ['acc_x','acc_y','acc_z','gy_x','gy_y','gy_z']\n",
        "      \n",
        "      # print(total_train_df.describe())\n",
        "        \n",
        "      ranges, windows_df = windows(total_train_df, window_size, overlap)\n",
        "      for ran in ranges:\n",
        "          l,r = ran.split(':')\n",
        "          df_range = windows_df.iloc[int(l): int(r)]\n",
        "          if int(r) > len(windows_df):\n",
        "              break\n",
        "          x_values = df_range.values\n",
        "\n",
        "          X.append(x_values)\n",
        "      # print(len(X), np.array(X).shape)\n",
        "\n",
        "      X = np.array(X, dtype=\"float32\")\n",
        "\n",
        "      actual_dataset = TerrainDataset2(X)\n",
        "      actual_dataset_loader = torch.utils.data.DataLoader(actual_dataset, batch_size = len(y_time_df.values), shuffle=False)\n",
        "\n",
        "      X = torch.from_numpy(X)\n",
        "\n",
        "      model1 = BiLSTM()\n",
        "      model1.load_state_dict(torch.load(model1_path))\n",
        "      for data in actual_dataset_loader:\n",
        "          pred1 = model1(data)\n",
        "          _, pred = torch.max(pred1, 1) \n",
        "      pred1 = get_pred(pred, len(y_time_df.values), window_size)\n",
        "      print(len(pred1))\n",
        "\n",
        "\n",
        "      model2 = BiLSTM()\n",
        "      model2.load_state_dict(torch.load(model2_path))\n",
        "      for data in actual_dataset_loader:\n",
        "          pred2 = model1(data)\n",
        "          _, pred = torch.max(pred2, 1) \n",
        "      pred2 = get_pred(pred, len(y_time_df.values), window_size)\n",
        "      print(len(pred2))\n",
        "\n",
        "      print(classification_report(y_true=pred1, y_pred=pred2))\n",
        "\n",
        "      cm = confusion_matrix(y_true=pred1, y_pred=pred2)\n",
        "      cmd = ConfusionMatrixDisplay(cm, display_labels=[\"standing / walking\", \"going downstairs\",\"going upstairs\", \"walking on grass\"])\n",
        "      cmd #.plot()\n",
        "\n",
        "      break\n",
        "\n",
        "model_path1 = r\"C:\\Users\\sumuk\\OneDrive\\Desktop\\NCSU_related\\Courses_and_stuff\\Courses_and_stuff\\NCSU_courses_and_books\\ECE_542\\CompetitiveProject\\Models\\BiLSTM_new_splits_2\\model_0.41.pt\"\n",
        "model_path2 = r\"C:\\Users\\sumuk\\OneDrive\\Desktop\\NCSU_related\\Courses_and_stuff\\Courses_and_stuff\\NCSU_courses_and_books\\ECE_542\\CompetitiveProject\\Models\\decay_rate\\model_1e7.pt\"\n",
        "make_dataset_with_norm(true_test_files, model1_path=model_path1, model2_path=model_path2, window_size=60, overlap=60, per_subject_norm=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prediction generation for test files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "S_PATH = \"dataset/ECE542_sp2022_Project_TerrainRecognition/\"  #'/content/drive/MyDrive/ECE542/CompetitiveProject/ECE542_sp2022_Project_TerrainRecognition'\n",
        "TEST_PATH = S_PATH + \"./TestData/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "true_test_files = sorted(os.listdir(TEST_PATH))\n",
        "target_path = r\"C:\\Users\\sumuk\\OneDrive\\Desktop\\NCSU_related\\Courses_and_stuff\\Courses_and_stuff\\NCSU_courses_and_books\\ECE_542\\CompetitiveProject\\Models\\Trial_2\\predictions_0.27\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TerrainDataset2(Dataset):\n",
        "  def __init__(self, data):\n",
        "    self.data = np.array(data)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.data[index]\n",
        "\n",
        "\n",
        "def get_pred(y_pred, window_size=60):\n",
        "\n",
        "  y_pred_total = []\n",
        "\n",
        "  for i in range(0,len(y_pred),4):\n",
        "    y_pred_total.extend([Counter(y_pred[i:i+4]).most_common(1)[0][0]])\n",
        "\n",
        "  return y_pred_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_actual_results(true_test_files, model, window_size=60, per_subject_norm=False, per_session_norm=True):\n",
        "  predictions = []\n",
        "  for i in range(0, len(true_test_files), 3):\n",
        "      X = []\n",
        "      x_time, x, y_time = true_test_files[i: i + 3]\n",
        "      train_df = None\n",
        "      total_train_df = pd.DataFrame()\n",
        "      \n",
        "\n",
        "      #X_features\n",
        "      x_time_df = pd.read_csv(TEST_PATH + x_time, header=None)\n",
        "      x_time_df.astype('float64')\n",
        "      x_df = pd.read_csv(TEST_PATH + x, header=None)\n",
        "      x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
        "      x_combined = x_combined.rename({0:'timestamp'}, axis='columns')\n",
        "      x_combined.set_index('timestamp', inplace=True)\n",
        "\n",
        "      #Y_labels\n",
        "      y_time_df = pd.read_csv(TEST_PATH + y_time , header=None)\n",
        "      y_time_df.astype('float64')\n",
        "      train_df = x_combined.rename({'1':'acc_x',2:'acc_y',3:'acc_z',4:'gy_x',5:'gy_y',6:'gy_z'}, axis='columns')\n",
        "\n",
        "        \n",
        "      if total_train_df.empty:\n",
        "        total_train_df = train_df\n",
        "      else:\n",
        "        total_train_df = pd.concat([total_train_df, train_df], ignore_index = True)\n",
        "        \n",
        "\n",
        "      if per_subject_norm or per_session_norm:   # after collating subject records, apply standard scalar to normalize subject data\n",
        "        temp_df = total_train_df.copy()\n",
        "        scaled_features = StandardScaler().fit_transform(temp_df.values)\n",
        "        total_train_df = pd.DataFrame(scaled_features, index=temp_df.index, columns=temp_df.columns)\n",
        "        # print(key, scaled_features_df.shape, labels.shape)\n",
        "        # print(key, total_train_df.shape)\n",
        "        total_train_df.columns = ['acc_x','acc_y','acc_z','gy_x','gy_y','gy_z']\n",
        "      \n",
        "      # print(total_train_df.describe())\n",
        "      print(total_train_df.shape)\n",
        "\n",
        "      addl_rows_df = pd.DataFrame(total_train_df.iloc[-window_size:])\n",
        "      total_train_df =  pd.concat([total_train_df, addl_rows_df], ignore_index = True)\n",
        "      for i in range(len(total_train_df) - window_size):\n",
        "          df_range = total_train_df.iloc[i:i+window_size]\n",
        "          x_values = df_range.values\n",
        "          X.append(x_values)\n",
        "          \n",
        "        \n",
        "      X = np.array(X, dtype=\"float32\")\n",
        "\n",
        "      actual_dataset = TerrainDataset2(X)\n",
        "      actual_dataset_loader = torch.utils.data.DataLoader(actual_dataset, batch_size = 1280, shuffle=False)\n",
        "\n",
        "      X = torch.from_numpy(X)\n",
        "\n",
        "      total_pred = []\n",
        "\n",
        "      for data in actual_dataset_loader:\n",
        "          pred1 = model(data)\n",
        "          _, pred = torch.max(pred1, 1)\n",
        "          total_pred.extend(list(map(int,pred)))\n",
        "      total_pred = get_pred(total_pred, len(y_time_df.values))\n",
        "      filename = path.join(target_path,(x_time.split('__')[0] + '__y.csv'))\n",
        "      y_df = pd.DataFrame(total_pred)\n",
        "      y_df.to_csv(filename, index=False, header=None)\n",
        "\n",
        "\n",
        "      predictions.append(pred1)\n",
        "\n",
        "\n",
        "  return predictions\n",
        "\n",
        "\n",
        "model1 = BiLSTM()\n",
        "model1.load_state_dict(torch.load(r'C:\\Users\\sumuk\\OneDrive\\Desktop\\NCSU_related\\Courses_and_stuff\\Courses_and_stuff\\NCSU_courses_and_books\\ECE_542\\CompetitiveProject\\Models\\Trial_2\\model_0.27_focal.pt'))\n",
        "model1.eval()\n",
        "\n",
        "\n",
        "predictions = get_actual_results(true_test_files, model1, window_size=60,per_subject_norm=False, per_session_norm=True)\n",
        "print(len(predictions))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Discrepancy checker - File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_files_folder = r\"C:\\Users\\sumuk\\OneDrive\\Desktop\\NCSU_related\\Courses_and_stuff\\Courses_and_stuff\\NCSU_courses_and_books\\ECE_542\\CompetitiveProject\\Results_from_Sai\"\n",
        "# predicted_files_folder = r\"C:\\Users\\sumuk\\OneDrive\\Desktop\\NCSU_related\\Courses_and_stuff\\Courses_and_stuff\\NCSU_courses_and_books\\ECE_542\\CompetitiveProject\\Models\\Trial_2\\predictions_60\"\n",
        "predicted_files_folder = r\"C:\\Users\\sumuk\\OneDrive\\Desktop\\NCSU_related\\Courses_and_stuff\\Courses_and_stuff\\NCSU_courses_and_books\\ECE_542\\CompetitiveProject\\Models\\Trial_2\\predictions_0.27\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_files_path = [os.path.join(target_files_folder, i) for i in sorted(os.listdir(target_files_folder))]\n",
        "predicted_files_path = [os.path.join(predicted_files_folder, i) for i in sorted(os.listdir(predicted_files_folder))]\n",
        "target_files = [pd.read_csv(i, header=None) for i in target_files_path]\n",
        "predicted_files = [pd.read_csv(i, header=None) for i in predicted_files_path]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[len(i) for i in predicted_files], [len(i) for i in target_files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cms = []\n",
        "for i, j in zip(target_files, predicted_files):\n",
        "    target_values = list(i.values)\n",
        "    predicted_values = list(j.values)\n",
        "    print(len(target_values), len(predicted_values))\n",
        "    cms.append(confusion_matrix(y_true=target_values, y_pred=predicted_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cms[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cms[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cms[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cms[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "AH5UcZtae3cB",
        "hx8vaFTTh1Eg"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
