{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p6HBsAqMjAKA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, InputLayer, Dropout, Conv1D, MaxPool1D, Input, LayerNormalization, Reshape, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.keras import regularizers\n",
        "from sklearn.utils import class_weight\n",
        "import statistics as st\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from numpy import genfromtxt\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from numpy import dstack\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.regularizers import L2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FclKX0ZnkMTB",
        "outputId": "fa291b2c-e375-456f-a127-c6546fa58b39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Eb46_qjAKB"
      },
      "source": [
        "# Dataset preparation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "kt7zYT8FjAKC"
      },
      "outputs": [],
      "source": [
        "S_PATH = '/content/drive/MyDrive/ECE542_sp2022_Project_TerrainRecognition'\n",
        "PATH = S_PATH +'/TrainingData/'\n",
        "TEST_PATH = S_PATH + \"/TestData/\"\n",
        "test_subject = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bvWzamnOjAKC"
      },
      "outputs": [],
      "source": [
        "files = []\n",
        "for filename in os.listdir(PATH):\n",
        "    files.append(filename)\n",
        "files = sorted(files, key = lambda x: (int(x.split('_')[1]),int(x.split('_')[2]), x.split('_')[4] ))\n",
        "files_train = list(filter(lambda x: int(x.split('_')[1]) not in  [test_subject], files))\n",
        "files_train = [os.path.join(PATH, i) for i in files_train]\n",
        "files_test = list(filter(lambda x: int(x.split('_')[1]) == test_subject, files))\n",
        "files_test = [os.path.join(PATH, i) for i in files_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ohvFCG3HjAKD"
      },
      "outputs": [],
      "source": [
        "files_train_x_path = files_train[1::4]\n",
        "files_test_x_path = files_test[1::4]\n",
        "files_train_y_path = files_train[3::4]\n",
        "files_test_y_path = files_test[3::4]\n",
        "files_train_x = [pd.read_csv(i) for i in files_train_x_path]\n",
        "files_test_x = [pd.read_csv(i) for i in files_test_x_path]\n",
        "files_train_y = [pd.read_csv(i) for i in files_train_y_path]\n",
        "files_test_y = [pd.read_csv(i) for i in files_test_y_path]\n",
        "# files_train_x, files_test_x, files_train_y, files_test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F0Tm4uMHjAKD"
      },
      "outputs": [],
      "source": [
        "def generate_splits(all_files_paths, all_files):\n",
        "    continue_loop = True\n",
        "    threshold = 0.005\n",
        "    all_files = pd.concat(all_files)\n",
        "    _, files_split_dist = np.unique(all_files, return_counts=True)\n",
        "    files_split_dist = np.divide(files_split_dist, np.max(files_split_dist))\n",
        "    while continue_loop:\n",
        "        random_seed = np.random.randint(0, (2 ** 31) - 1)\n",
        "        np.random.seed(random_seed)\n",
        "        random.seed(random_seed)\n",
        "        files_train_y_split_1 = random.sample(all_files_paths, int(len(all_files_paths) * 0.8))\n",
        "        files_train_y_split_2 = [i for i in all_files_paths if i not in files_train_y_split_1]\n",
        "        files_split_1_val = pd.concat([pd.read_csv(i , header=None) for i in files_train_y_split_1])\n",
        "        files_split_2_val = pd.concat([pd.read_csv(i , header=None) for i in files_train_y_split_2])\n",
        "        _, files_split_1_dist = np.unique(files_split_1_val, return_counts=True)\n",
        "        files_split_1_dist = np.divide(files_split_1_dist, np.max(files_split_1_dist))\n",
        "        _, files_split_2_dist = np.unique(files_split_2_val, return_counts=True)\n",
        "        files_split_2_dist = np.divide(files_split_2_dist, np.max(files_split_2_dist))\n",
        "        if ((files_split_dist[0] - threshold <= files_split_1_dist[0] <= files_split_dist[0] + threshold) and\n",
        "            (files_split_dist[1] - threshold <= files_split_1_dist[1] <= files_split_dist[1] + threshold) and\n",
        "            (files_split_dist[2] - threshold <= files_split_1_dist[2] <= files_split_dist[2] + threshold) and\n",
        "            (files_split_dist[3] - threshold <= files_split_1_dist[3] <= files_split_dist[3] + threshold) and\n",
        "            (files_split_dist[0] - threshold <= files_split_2_dist[0] <= files_split_dist[0] + threshold) and\n",
        "            (files_split_dist[1] - threshold <= files_split_2_dist[1] <= files_split_dist[1] + threshold) and\n",
        "            (files_split_dist[2] - threshold <= files_split_2_dist[2] <= files_split_dist[2] + threshold) and\n",
        "            (files_split_dist[3] - threshold <= files_split_2_dist[3] <= files_split_dist[3] + threshold)):\n",
        "                print(files_split_1_dist, files_split_2_dist, files_split_dist)\n",
        "                return sorted(files_train_y_split_1), sorted(files_train_y_split_2), random_seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAhKuz1TjAKE",
        "outputId": "3248bd49-286a-42f7-f0cd-95c742b63ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.         0.05441013 0.0761707  0.21220472] [1.         0.06035783 0.06987407 0.20947956] [1.         0.0557347  0.07478412 0.21162566]\n"
          ]
        }
      ],
      "source": [
        "y_train_files, y_val_files, random_seed = generate_splits(files_train_y_path, files_train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9R8Xk-QjAKE"
      },
      "outputs": [],
      "source": [
        "y_train_file_names = [\"_\".join(i.split(\"/\")[-1].split(\"__\")[: -1]) for i in y_train_files]\n",
        "y_val_file_names = [\"_\".join(i.split(\"/\")[-1].split(\"__\")[: -1]) for i in y_val_files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUl3u0O2jAKF"
      },
      "outputs": [],
      "source": [
        "x_train_files = sorted([i for i in files_train_x_path if \"_\".join(i.split(\"/\")[-1].split(\"__\")[: -1]) in y_train_file_names])\n",
        "x_val_files = sorted([i for i in files_train_x_path if \"_\".join(i.split(\"/\")[-1].split(\"__\")[: -1]) in y_val_file_names])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1llU6c8CjAKF"
      },
      "outputs": [],
      "source": [
        "def most_frequent(List):\n",
        "    occurence_count = Counter(List)\n",
        "    return occurence_count.most_common(1)[0][0]\n",
        "\n",
        "\n",
        "def mode(a, axis=0):\n",
        "    scores = np.unique(np.ravel(a))       # get ALL unique values\n",
        "    testshape = list(a.shape)\n",
        "    testshape[axis] = 1\n",
        "    oldmostfreq = np.zeros(testshape)\n",
        "    oldcounts = np.zeros(testshape)\n",
        "\n",
        "    for score in scores:\n",
        "        template = (a == score)\n",
        "        counts = np.expand_dims(np.sum(template, axis),axis)\n",
        "        mostfrequent = np.where(counts > oldcounts, score, oldmostfreq)\n",
        "        oldcounts = np.maximum(counts, oldcounts)\n",
        "        oldmostfreq = mostfrequent\n",
        "\n",
        "    return mostfrequent, oldcounts\n",
        "\n",
        "\n",
        "def preprocessing(x_files, y_files, window_size=30):\n",
        "    final_x = []\n",
        "    final_y = []\n",
        "    for i in range(len(y_files)):\n",
        "        X_df = pd.read_csv(x_files[i])\n",
        "        y_df = pd.read_csv(y_files[i])\n",
        "        a = []\n",
        "        for i in range(0, len(y_df)):\n",
        "            a += [y_df['0'][i]] * 4\n",
        "        upsampled_df = pd.DataFrame(a)\n",
        "        diff = X_df.shape[0] - upsampled_df.shape[0]\n",
        "        X_df = X_df.iloc[:-diff,:]\n",
        "\n",
        "        sc=StandardScaler()\n",
        "        X_df=sc.fit_transform(X_df)\n",
        "\n",
        "        x_arr=[]\n",
        "        y_arr=[]\n",
        "\n",
        "        for j in range(len(X_df)-30):\n",
        "            x_arr.append(X_df[j: (j + window_size)]) \n",
        "            y_arr.append(mode(upsampled_df.iloc[j: (j + window_size)])[0])\n",
        "        final_x.append(np.array(x_arr))\n",
        "        final_y.append(np.array(y_arr).reshape(-1,1))\n",
        "    return final_x, final_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UYcqehR-jAKF"
      },
      "outputs": [],
      "source": [
        "# trainX, trainY = preprocessing(x_train_files, y_train_files)\n",
        "# trainX, trainY = np.concatenate(trainX), np.concatenate(trainY)\n",
        "# valX, valY = preprocessing(x_train_files, y_train_files)\n",
        "# valX, valY = np.concatenate(valX), np.concatenate(valY)\n",
        "testX, testY = preprocessing(files_test_x_path, files_test_y_path)\n",
        "testX, testY = np.concatenate(testX), np.concatenate(testY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparing the dataset for with only labels 0 and 3"
      ],
      "metadata": {
        "id": "gcBo7R6m-opd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_indexes = np.where((trainY == 0) | (trainY == 3))[0]\n",
        "trainY_binary = trainY[train_indexes]\n",
        "trainX_binary = trainX[train_indexes]\n",
        "val_indexes = np.where((valY == 0) | (valY == 3))[0]\n",
        "valY_binary = valY[val_indexes]\n",
        "valX_binary = valX[val_indexes]\n",
        "test_indexes = np.where((testY == 0) | (testY == 3))[0]\n",
        "testY_binary = testY[test_indexes]\n",
        "testX_binary = testX[test_indexes]"
      ],
      "metadata": {
        "id": "EuzmCQEM89Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(trainY_binary,return_counts=True)\n",
        "class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(trainY_binary), y=trainY_binary.ravel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBBCbKJAAENe",
        "outputId": "b3d1b8e2-8aa8-4d26-d39c-c3646cdc58ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.60600922, 2.8582855 ])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTtUzJbojAKG",
        "outputId": "a1062b94-c76a-48e0-9e13-64fb5278a055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.6060092174990317, 1: 0}\n"
          ]
        }
      ],
      "source": [
        "label_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(trainY_binary), y=trainY_binary.ravel())\n",
        "label_weights = {i:label_weights[i] for i in range(len(label_weights))} # Create dictionary\n",
        "print(label_weights)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_weights_binary = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(trainY_binary), y=trainY_binary.ravel())\n",
        "label_weights_binary = {0:label_weights_binary[0], 1:0, 2:0, 3: label_weights_binary[1]} # Create dictionary\n",
        "print(label_weights_binary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmLbaiNpBO6w",
        "outputId": "1e02673b-6890-4be9-bcb3-dd70e94f8b05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.6060092174990317, 1: 0, 2: 0, 3: 2.8582854953370784}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlDvpfKCjAKG"
      },
      "outputs": [],
      "source": [
        "hot_y_train = np_utils.to_categorical(trainY_binary)\n",
        "hot_y_val = np_utils.to_categorical(valY_binary)\n",
        "# hot_y_test = np_utils.to_categorical(testY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44CAILcBjAKG"
      },
      "source": [
        "# Model training and inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fIdSTRIqjAKG"
      },
      "outputs": [],
      "source": [
        "def simpleLSTM(timestep, features, n_outputs, hidden_units=125):\n",
        "    model_LSTM = Sequential()\n",
        "    model_LSTM.add(InputLayer(input_shape=(timestep, features)))\n",
        "    model_LSTM.add(LSTM(units=hidden_units, activation='relu'))\n",
        "    model_LSTM.add(Dropout(0.5))\n",
        "    model_LSTM.add(Dense(units = hidden_units, activation = 'relu'))\n",
        "    model_LSTM.add(Dense(n_outputs, activation='softmax')) # output layer\n",
        "    return model_LSTM\n",
        "\n",
        "\n",
        "def simpleLSTM2(timestep, features, n_outputs, hidden_units=125):\n",
        "    x_in = Input(shape=(timestep, features))\n",
        "    x = LSTM(units=hidden_units, activation='relu', kernel_regularizer=L2(0.001), recurrent_regularizer=L2(0.001), bias_regularizer=L2(0.001))(x_in)\n",
        "    x = LayerNormalization()(x) \n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(units = hidden_units, activation = 'relu', kernel_regularizer=L2(0.001), bias_regularizer=L2(0.001))(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dense(n_outputs, activation='softmax')(x)\n",
        "    model_LSTM = Model(inputs=x_in, outputs=x)\n",
        "    return model_LSTM\n",
        "\n",
        "\n",
        "\n",
        "def LSTMConv(timestep, features, n_outputs, hidden_units=125, filters=64, kernel_size=1):\n",
        "    x_in = Input(shape=(timestep, features))\n",
        "    x = LSTM(units=hidden_units, activation='relu')(x_in)\n",
        "    x = LayerNormalization()(x) \n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Reshape((1, 1, hidden_units))(x)\n",
        "    print(x.shape)\n",
        "    x = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same')(x) \n",
        "    x = Flatten()(x) \n",
        "    x = Dense(units=hidden_units, activation='relu')(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dense(n_outputs, activation='softmax')(x)\n",
        "    model = Model(inputs=x_in, outputs=x)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "edM7B0V5jAKH"
      },
      "outputs": [],
      "source": [
        "# timestep, features, n_outputs = trainX_binary.shape[1], trainX_binary.shape[2], 4\n",
        "# model = simpleLSTM(timestep, features, n_outputs, 125)\n",
        "model = simpleLSTM2(30, 6, 4, 125)\n",
        "# model = LSTMConv(timestep, features, n_outputs, hidden_units=125, filters=64, kernel_size=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gifE5dMOjAKH"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=3, min_lr=0.000001\n",
        "    ),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=8, verbose=1),\n",
        "]\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.08)\n",
        "optimizer= Adam(learning_rate=0.001)\n",
        "metrics=['accuracy']\n",
        "model.compile(optimizer, loss, metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imtVyMUNjAKH",
        "outputId": "04abb78a-54d5-4f8e-e2b2-e8fe3991a34a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3264/3264 [==============================] - 202s 61ms/step - loss: 0.6700 - accuracy: 0.8279 - val_loss: 0.4804 - val_accuracy: 0.9155 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "3264/3264 [==============================] - 194s 59ms/step - loss: 0.5070 - accuracy: 0.8932 - val_loss: 0.4794 - val_accuracy: 0.9073 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "3264/3264 [==============================] - 212s 65ms/step - loss: 0.4764 - accuracy: 0.9048 - val_loss: 0.4146 - val_accuracy: 0.9365 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "3264/3264 [==============================] - 214s 65ms/step - loss: 0.4614 - accuracy: 0.9110 - val_loss: 0.4251 - val_accuracy: 0.9280 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "3264/3264 [==============================] - 212s 65ms/step - loss: 0.4542 - accuracy: 0.9136 - val_loss: 0.4061 - val_accuracy: 0.9398 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "3264/3264 [==============================] - 213s 65ms/step - loss: 0.4483 - accuracy: 0.9158 - val_loss: 0.4092 - val_accuracy: 0.9354 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "3264/3264 [==============================] - 194s 59ms/step - loss: 0.4420 - accuracy: 0.9196 - val_loss: 0.4007 - val_accuracy: 0.9418 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "3264/3264 [==============================] - 211s 65ms/step - loss: 0.4384 - accuracy: 0.9206 - val_loss: 0.4389 - val_accuracy: 0.9216 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "3264/3264 [==============================] - 211s 65ms/step - loss: 0.4346 - accuracy: 0.9237 - val_loss: 0.4861 - val_accuracy: 0.8976 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "3264/3264 [==============================] - 212s 65ms/step - loss: 0.4319 - accuracy: 0.9241 - val_loss: 0.4047 - val_accuracy: 0.9417 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "3264/3264 [==============================] - 211s 65ms/step - loss: 0.4105 - accuracy: 0.9332 - val_loss: 0.3768 - val_accuracy: 0.9506 - lr: 5.0000e-04\n",
            "Epoch 12/50\n",
            "3264/3264 [==============================] - 211s 65ms/step - loss: 0.4062 - accuracy: 0.9342 - val_loss: 0.3898 - val_accuracy: 0.9461 - lr: 5.0000e-04\n",
            "Epoch 13/50\n",
            "3264/3264 [==============================] - 211s 65ms/step - loss: 0.4033 - accuracy: 0.9361 - val_loss: 0.3924 - val_accuracy: 0.9432 - lr: 5.0000e-04\n",
            "Epoch 14/50\n",
            "3264/3264 [==============================] - 192s 59ms/step - loss: 0.4025 - accuracy: 0.9360 - val_loss: 0.4032 - val_accuracy: 0.9392 - lr: 5.0000e-04\n",
            "Epoch 15/50\n",
            "3264/3264 [==============================] - 217s 66ms/step - loss: 0.3888 - accuracy: 0.9416 - val_loss: 0.3866 - val_accuracy: 0.9448 - lr: 2.5000e-04\n",
            "Epoch 16/50\n",
            "3264/3264 [==============================] - 216s 66ms/step - loss: 0.3868 - accuracy: 0.9422 - val_loss: 0.3677 - val_accuracy: 0.9521 - lr: 2.5000e-04\n",
            "Epoch 17/50\n",
            "3264/3264 [==============================] - 193s 59ms/step - loss: 0.3852 - accuracy: 0.9426 - val_loss: 0.3713 - val_accuracy: 0.9516 - lr: 2.5000e-04\n",
            "Epoch 18/50\n",
            "3264/3264 [==============================] - 211s 65ms/step - loss: 0.3836 - accuracy: 0.9432 - val_loss: 0.3610 - val_accuracy: 0.9571 - lr: 2.5000e-04\n",
            "Epoch 19/50\n",
            "3264/3264 [==============================] - 212s 65ms/step - loss: 0.3828 - accuracy: 0.9438 - val_loss: 0.3570 - val_accuracy: 0.9583 - lr: 2.5000e-04\n",
            "Epoch 20/50\n",
            "3264/3264 [==============================] - 213s 65ms/step - loss: 0.3816 - accuracy: 0.9437 - val_loss: 0.3844 - val_accuracy: 0.9457 - lr: 2.5000e-04\n",
            "Epoch 21/50\n",
            "3264/3264 [==============================] - 214s 66ms/step - loss: 0.3808 - accuracy: 0.9444 - val_loss: 0.3545 - val_accuracy: 0.9600 - lr: 2.5000e-04\n",
            "Epoch 22/50\n",
            "3264/3264 [==============================] - 214s 65ms/step - loss: 0.3798 - accuracy: 0.9447 - val_loss: 0.3701 - val_accuracy: 0.9525 - lr: 2.5000e-04\n",
            "Epoch 23/50\n",
            "3264/3264 [==============================] - 212s 65ms/step - loss: 0.3804 - accuracy: 0.9443 - val_loss: 0.3616 - val_accuracy: 0.9563 - lr: 2.5000e-04\n",
            "Epoch 24/50\n",
            "3264/3264 [==============================] - 193s 59ms/step - loss: 0.3778 - accuracy: 0.9460 - val_loss: 0.3611 - val_accuracy: 0.9568 - lr: 2.5000e-04\n",
            "Epoch 25/50\n",
            "3264/3264 [==============================] - 212s 65ms/step - loss: 0.3700 - accuracy: 0.9493 - val_loss: 0.3602 - val_accuracy: 0.9570 - lr: 1.2500e-04\n",
            "Epoch 26/50\n",
            "3264/3264 [==============================] - 212s 65ms/step - loss: 0.3686 - accuracy: 0.9495 - val_loss: 0.3773 - val_accuracy: 0.9480 - lr: 1.2500e-04\n",
            "Epoch 27/50\n",
            "3264/3264 [==============================] - 212s 65ms/step - loss: 0.3682 - accuracy: 0.9491 - val_loss: 0.3734 - val_accuracy: 0.9502 - lr: 1.2500e-04\n",
            "Epoch 28/50\n",
            "3264/3264 [==============================] - 212s 65ms/step - loss: 0.3635 - accuracy: 0.9517 - val_loss: 0.3736 - val_accuracy: 0.9491 - lr: 6.2500e-05\n",
            "Epoch 29/50\n",
            "3264/3264 [==============================] - 214s 66ms/step - loss: 0.3624 - accuracy: 0.9523 - val_loss: 0.3634 - val_accuracy: 0.9545 - lr: 6.2500e-05\n",
            "Epoch 30/50\n",
            "3264/3264 [==============================] - 213s 65ms/step - loss: 0.3620 - accuracy: 0.9523 - val_loss: 0.3530 - val_accuracy: 0.9598 - lr: 6.2500e-05\n",
            "Epoch 31/50\n",
            "3264/3264 [==============================] - 192s 59ms/step - loss: 0.3615 - accuracy: 0.9524 - val_loss: 0.3673 - val_accuracy: 0.9523 - lr: 6.2500e-05\n",
            "Epoch 32/50\n",
            "3264/3264 [==============================] - 213s 65ms/step - loss: 0.3610 - accuracy: 0.9525 - val_loss: 0.3566 - val_accuracy: 0.9574 - lr: 6.2500e-05\n",
            "Epoch 33/50\n",
            "3263/3264 [============================>.] - ETA: 0s - loss: 0.3606 - accuracy: 0.9526"
          ]
        }
      ],
      "source": [
        "history = model.fit(trainX_binary, hot_y_train, epochs = 50, batch_size = 256,\n",
        "                    validation_data = (valX_binary, hot_y_val), class_weight = label_weights_binary,\n",
        "                    verbose = 1, shuffle = True, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVmH1F5NjAKH"
      },
      "outputs": [],
      "source": [
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'], color='red')\n",
        "plt.plot(history.history['val_accuracy'], color='blue')\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'], color='red')\n",
        "plt.plot(history.history['val_loss'], color='blue')\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_main = simpleLSTM2(30, 6, 4, 125)\n",
        "model_main.load_weights(\"./best_model_main.h5\")\n",
        "model_binary = simpleLSTM2(30, 6, 4, 125)\n",
        "model_binary.load_weights(\"./best_model.h5\")\n",
        "y_pred_main = model_main.predict(testX, batch_size = 256, verbose = 1)\n",
        "y_test = np.argmax(y_pred_main, axis = 1)\n",
        "y_pred_bin = model_binary.predict(testX, batch_size = 256, verbose = 1)\n",
        "y_test_bin = np.argmax(y_pred_bin, axis = 1)\n",
        "y_test_ensemble = []\n",
        "for index, value in enumerate(y_test):\n",
        "  if value == 3:\n",
        "    if value == y_test_bin[index]:\n",
        "      y_test_ensemble.append(value)\n",
        "    else:\n",
        "      y_test_ensemble.append(y_test_bin[index])\n",
        "  else:\n",
        "    y_test_ensemble.append(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI58UuHOCcSL",
        "outputId": "056e4c6f-6cfe-4cfa-a2e3-8a19d53f6eab"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "597/597 [==============================] - 43s 71ms/step\n",
            "597/597 [==============================] - 42s 69ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_test_ensemble), len(y_test_bool)\n",
        "np.unique(y_test_ensemble)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI63T0g0DL-c",
        "outputId": "7c05d82b-be97-4a18-e455-639828577b7b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(152714, 152714)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(testY, y_test_ensemble))   # Ensemble model test performance\n",
        "print(confusion_matrix(testY, y_test_ensemble))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPZ94LxWEg_m",
        "outputId": "9fafbf12-2b0b-4600-912d-f7a9942d0ed8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.96      0.96    121102\n",
            "         1.0       0.82      0.98      0.89      5824\n",
            "         2.0       0.88      0.99      0.93      6807\n",
            "         3.0       0.85      0.72      0.78     18981\n",
            "\n",
            "    accuracy                           0.94    152714\n",
            "   macro avg       0.88      0.91      0.89    152714\n",
            "weighted avg       0.93      0.94      0.93    152714\n",
            "\n",
            "[[116782   1065    827   2428]\n",
            " [   123   5685      2     14]\n",
            " [    61      0   6728     18]\n",
            " [  5086    209     66  13620]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "87LdL9kqjAKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a073614b-3618-4756-877b-3441a70789a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "597/597 [==============================] - 40s 67ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.92      0.95    121102\n",
            "         1.0       0.82      0.98      0.89      5824\n",
            "         2.0       0.88      0.99      0.93      6807\n",
            "         3.0       0.69      0.85      0.76     18981\n",
            "\n",
            "    accuracy                           0.92    152714\n",
            "   macro avg       0.84      0.93      0.88    152714\n",
            "weighted avg       0.93      0.92      0.92    152714\n",
            "\n",
            "[[111886   1065    827   7324]\n",
            " [   109   5685      2     28]\n",
            " [    44      0   6728     35]\n",
            " [  2544    209     66  16162]]\n"
          ]
        }
      ],
      "source": [
        "model_weights = r\"./best_model_main.h5\"   #Main Model Test Performance\n",
        "model.load_weights(model_weights)\n",
        "y_pred = model.predict(testX, batch_size = 256, verbose = 1)\n",
        "y_test_bool = np.argmax(y_pred, axis = 1)\n",
        "print(classification_report(testY, y_test_bool))\n",
        "print(confusion_matrix(testY, y_test_bool))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5qTeVpzjAKI"
      },
      "source": [
        "# Prediction generation on actual test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "LGIeSV2TjAKI"
      },
      "outputs": [],
      "source": [
        "# test_dataset_folder = r\"C:\\Users\\sumuk\\OneDrive\\Desktop\\NCSU_related\\Courses_and_stuff\\Courses_and_stuff\\NCSU_courses_and_books\\ECE_542\\CompetitiveProject\\terrain_prediction\\dataset\\ECE542_sp2022_Project_TerrainRecognition\\TestData\"\n",
        "# save_folder = r\"C:\\Users\\sumuk\\OneDrive\\Desktop\\NCSU_related\\Courses_and_stuff\\Courses_and_stuff\\NCSU_courses_and_books\\ECE_542\\CompetitiveProject\\Models\\tf_experiments\\6\"\n",
        "test_dataset_folder = TEST_PATH\n",
        "save_folder = \".\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ensemble(model_main, model_binary,test_dataset_folder, file_name_x, file_name_y, save_name):\n",
        "    input_data = pd.read_csv(os.path.join(test_dataset_folder, file_name_x))\n",
        "    sc = StandardScaler()\n",
        "    df = sc.fit_transform(input_data)\n",
        "    y_frame = pd.read_csv(os.path.join(test_dataset_folder, file_name_y),header=None)\n",
        "    addl_rows = y_frame.shape[0] * 4 - df.shape[0] + 30\n",
        "    addl_rows_df = pd.DataFrame(df[-addl_rows:])\n",
        "    df = pd.DataFrame(df)\n",
        "    df = df.append(addl_rows_df)\n",
        "    X_values = []\n",
        "    for i in range(0, len(df) - 30, 1):\n",
        "        value = df.iloc[i:(i + 30)].values\n",
        "        X_values.append(value)\n",
        "    X_test = np.array(X_values)\n",
        "    y_test = model_main.predict(X_test, batch_size = 256, verbose = 1)  # Main Model\n",
        "    y_test_main = np.argmax(y_test, axis = 1)\n",
        "    y_pred_bin = model_binary.predict(X_test, batch_size = 256, verbose = 1) # Binary Classification Model\n",
        "    y_test_bin = np.argmax(y_pred_bin, axis = 1)\n",
        "    y_test_ensemble = []\n",
        "    for index, value in enumerate(y_test_main):\n",
        "      if value == 3:\n",
        "        if value == y_test_bin[index]:\n",
        "          y_test_ensemble.append(value)\n",
        "        else:\n",
        "          y_test_ensemble.append(y_test_bin[index])\n",
        "      else:\n",
        "        y_test_ensemble.append(value)\n",
        "    y_test_ensemble = np.array(y_test_ensemble)\n",
        "    output_actual = []\n",
        "    for i in range(0, y_test_ensemble.shape[0], 4):\n",
        "        a = list(y_test_ensemble[i:i + 4])\n",
        "        output_actual.append(max(a, key = a.count))\n",
        "    y_actual = np.array(output_actual)\n",
        "    print(y_actual.size)\n",
        "    y_series = pd.Series(y_actual)\n",
        "    y_series.to_csv(os.path.join(save_folder, save_name),index=False, header=None)"
      ],
      "metadata": {
        "id": "f-5rPxQ9G8jj"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "eZu3LzUajAKI"
      },
      "outputs": [],
      "source": [
        "def predict(model, test_dataset_folder, file_name_x, file_name_y, save_name):\n",
        "    input_data = pd.read_csv(os.path.join(test_dataset_folder, file_name_x))\n",
        "    sc = StandardScaler()\n",
        "    df = sc.fit_transform(input_data)\n",
        "    y_frame = pd.read_csv(os.path.join(test_dataset_folder, file_name_y),header=None)\n",
        "    addl_rows = y_frame.shape[0] * 4 - df.shape[0] + 30\n",
        "    addl_rows_df = pd.DataFrame(df[-addl_rows:])\n",
        "    df = pd.DataFrame(df)\n",
        "    df = df.append(addl_rows_df)\n",
        "    X_values = []\n",
        "    for i in range(0, len(df) - 30, 1):\n",
        "        value = df.iloc[i:(i + 30)].values\n",
        "        X_values.append(value)\n",
        "    X_test = np.array(X_values)\n",
        "    y_test = model.predict(X_test, batch_size = 256, verbose = 1)\n",
        "    y_test_class = np.argmax(y_test, axis = 1)\n",
        "    output_actual = []\n",
        "    for i in range(0, y_test_class.shape[0], 4):\n",
        "        a = list(y_test_class[i:i + 4])\n",
        "        output_actual.append(max(a, key = a.count))\n",
        "    y_actual = np.array(output_actual)\n",
        "    print(y_actual.size)\n",
        "    y_series = pd.Series(y_actual)\n",
        "    y_series.to_csv(os.path.join(save_folder, save_name),index=False, header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbhXyC1MjAKI"
      },
      "outputs": [],
      "source": [
        "model_weights = r\"C:\\Users\\sumuk\\OneDrive\\Desktop\\NCSU_related\\Courses_and_stuff\\Courses_and_stuff\\NCSU_courses_and_books\\ECE_542\\CompetitiveProject\\terrain_prediction\\best_model.h5\"\n",
        "model.load_weights(model_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original Model Predictions"
      ],
      "metadata": {
        "id": "vstODzzPH1zH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L97VwcdjAKJ"
      },
      "outputs": [],
      "source": [
        "predict(model, test_dataset_folder, \"subject_009_01__x.csv\", \"subject_009_01__y_time.csv\", \"subject_009_01__y.csv\")\n",
        "predict(model, test_dataset_folder, \"subject_010_01__x.csv\", \"subject_010_01__y_time.csv\", \"subject_010_01__y.csv\")\n",
        "predict(model, test_dataset_folder, \"subject_011_01__x.csv\", \"subject_011_01__y_time.csv\", \"subject_011_01__y.csv\")\n",
        "predict(model, test_dataset_folder, \"subject_012_01__x.csv\", \"subject_012_01__y_time.csv\", \"subject_012_01__y.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble Model Predictions"
      ],
      "metadata": {
        "id": "BFGucYdlH6sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_main = simpleLSTM2(30, 6, 4, 125)\n",
        "model_main.load_weights(\"./best_model_main.h5\")\n",
        "model_binary = simpleLSTM2(30, 6, 4, 125)\n",
        "model_binary.load_weights(\"./best_model.h5\")\n",
        "predict_ensemble(model_main,model_binary, test_dataset_folder, \"subject_009_01__x.csv\", \"subject_009_01__y_time.csv\", \"subject_009_01__y.csv\")\n",
        "predict_ensemble(model_main,model_binary, test_dataset_folder, \"subject_010_01__x.csv\", \"subject_010_01__y_time.csv\", \"subject_010_01__y.csv\")\n",
        "predict_ensemble(model_main,model_binary, test_dataset_folder, \"subject_011_01__x.csv\", \"subject_011_01__y_time.csv\", \"subject_011_01__y.csv\")\n",
        "predict_ensemble(model_main,model_binary, test_dataset_folder, \"subject_012_01__x.csv\", \"subject_012_01__y_time.csv\", \"subject_012_01__y.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-DWDW_2H9cT",
        "outputId": "24e72af6-b4ad-4e8d-8f9a-4e50f6bd8841"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-56-117376f459ca>:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(addl_rows_df)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "149/149 [==============================] - 12s 76ms/step\n",
            "149/149 [==============================] - 11s 68ms/step\n",
            "9498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-56-117376f459ca>:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(addl_rows_df)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "192/192 [==============================] - 13s 68ms/step\n",
            "192/192 [==============================] - 14s 70ms/step\n",
            "12270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-56-117376f459ca>:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(addl_rows_df)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "203/203 [==============================] - 14s 69ms/step\n",
            "203/203 [==============================] - 14s 70ms/step\n",
            "12940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-56-117376f459ca>:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(addl_rows_df)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "178/178 [==============================] - 12s 66ms/step\n",
            "178/178 [==============================] - 12s 70ms/step\n",
            "11330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdmmmdJTjAKJ"
      },
      "source": [
        "# compare 2 models (discrepency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGdN0JT7jAKJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20M_YOFZjAKJ"
      },
      "outputs": [],
      "source": [
        "# target_files_folder = r\"C:\\Users\\sumuk\\OneDrive\\Desktop\\NCSU_related\\Courses_and_stuff\\Courses_and_stuff\\NCSU_courses_and_books\\ECE_542\\CompetitiveProject\\Models\\C3.2_submission\"\n",
        "target_files_folder = r\"C:\\Users\\sumuk\\OneDrive\\Desktop\\NCSU_related\\Courses_and_stuff\\Courses_and_stuff\\NCSU_courses_and_books\\ECE_542\\CompetitiveProject\\Results_from_Sai\"\n",
        "predicted_files_folder = r\"C:\\Users\\sumuk\\OneDrive\\Desktop\\NCSU_related\\Courses_and_stuff\\Courses_and_stuff\\NCSU_courses_and_books\\ECE_542\\CompetitiveProject\\Models\\tf_experiments\\6\"\n",
        "# predicted_files_folder = r\"C:\\Users\\sumuk\\OneDrive\\Desktop\\NCSU_related\\Courses_and_stuff\\Courses_and_stuff\\NCSU_courses_and_books\\ECE_542\\CompetitiveProject\\Models\\C3.2_submission\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jiFKf8GjAKJ"
      },
      "outputs": [],
      "source": [
        "target_files_path = [os.path.join(target_files_folder, i) for i in sorted(os.listdir(target_files_folder))]\n",
        "predicted_files_path = [os.path.join(predicted_files_folder, i) for i in sorted(os.listdir(predicted_files_folder))]\n",
        "target_files = [pd.read_csv(i, header=None) for i in target_files_path]\n",
        "predicted_files = [pd.read_csv(i, header=None) for i in predicted_files_path]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_Z4bLomjAKJ"
      },
      "outputs": [],
      "source": [
        "[len(i) for i in predicted_files], [len(i) for i in target_files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDaGdy2ijAKK"
      },
      "outputs": [],
      "source": [
        "cms = []\n",
        "for i, j in zip(target_files, predicted_files):\n",
        "    target_values = list(i.values)\n",
        "    predicted_values = list(j.values)\n",
        "    print(len(target_values), len(predicted_values))\n",
        "    cms.append(confusion_matrix(y_true=target_values, y_pred=predicted_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGOSw_9djAKK"
      },
      "outputs": [],
      "source": [
        "cms[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGJwb58UjAKK"
      },
      "outputs": [],
      "source": [
        "cms[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dkAANNJjAKK"
      },
      "outputs": [],
      "source": [
        "cms[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS634eYSjAKK"
      },
      "outputs": [],
      "source": [
        "cms[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhSX-ZnOjAKK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "BRaTS2021_tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}