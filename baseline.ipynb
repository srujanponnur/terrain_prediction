{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./TrainingData/\"\n",
    "val_subject = 5\n",
    "test_subject = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for filename in os.listdir(PATH):\n",
    "    files.append(filename)\n",
    "files = sorted(files, key = lambda x: (int(x.split('_')[1]),int(x.split('_')[2]), x.split('_')[4] ))\n",
    "files_train = list(filter(lambda x: int(x.split('_')[1]) not in  [val_subject, test_subject], files))\n",
    "files_val = list(filter(lambda x: int(x.split('_')[1]) == val_subject, files))\n",
    "files_test = list(filter(lambda x: int(x.split('_')[1])== test_subject, files))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinating features and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train(files, X,Y):\n",
    "    for i in range(0, len(files), 4):\n",
    "        x_time, x, y_time, y = files[i: i + 4]\n",
    "        x_time_df = pd.read_csv(PATH + x_time , header=None)\n",
    "        x_df = pd.read_csv(PATH + x , header=None)\n",
    "        x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
    "        x_combined = x_combined.loc[range(1,len(x_combined), 4)].reset_index()  # down sampled the frequency\n",
    "        # print(x_combined.shape)\n",
    "        y_time_df = pd.read_csv(PATH + y_time , header=None)\n",
    "        y_df = pd.read_csv(PATH + y , header=None)\n",
    "        y_combined = pd.concat([y_time_df, y_df], axis=1, ignore_index=True)\n",
    "        # print(y_combined.shape)\n",
    "        train_df = pd.concat([x_combined, y_combined], axis=1, ignore_index=True)\n",
    "        train_df = train_df.drop(columns=[0, 1, 8])  # Dropping the time stamp\n",
    "        Y.extend(train_df[9].values)\n",
    "        X.extend(train_df.drop(columns=[9]).values)\n",
    "    print(len(X), len(Y))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335413 335413\n",
      "263333 263333\n",
      "33876 33876\n",
      "38204 38204\n",
      "Dataset Seperation Done\n"
     ]
    }
   ],
   "source": [
    "X_total, y_total = [], []\n",
    "make_train(files, X_total, y_total)\n",
    "X_train, y_train = [], []\n",
    "make_train(files_train, X_train, y_train)\n",
    "X_val, y_val = [], []\n",
    "make_train(files_val, X_val, y_val)\n",
    "X_test, y_test = [], []\n",
    "make_train(files_test, X_test, y_test)\n",
    "print(\"Dataset Seperation Done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not really required now, because testing is done using a specific subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
    "                                                    stratify=Y, \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.3403167275791302, 1: 5.962076616554972, 2: 4.445789438141545, 3: 1.494987056045054}\n"
     ]
    }
   ],
   "source": [
    "weight  = compute_class_weight(class_weight = 'balanced', classes = np.unique(y_train), y = np.array(y_train))\n",
    "weight_dict = {}\n",
    "for index, weight in enumerate(weight):\n",
    "    weight_dict[index] = float(weight)\n",
    "print(weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 161786, 3: 30446, 1: 9770, 2: 12221})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 500, max_depth = 4, max_features = 3, bootstrap = True, random_state = 42, class_weight=weight_dict).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_2 = RandomForestClassifier(n_estimators = 1000, max_depth = 10, max_features = 'auto', bootstrap = True, random_state = 42, class_weight=weight_dict).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: Counter({0: 15632, 3: 9744, 1: 8100, 2: 4728}), Actual: Counter({0: 30283, 3: 4750, 2: 1708, 1: 1463})\n"
     ]
    }
   ],
   "source": [
    "print(f'Predictions: {Counter(pred)}, Actual: {Counter(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.46      0.61     30283\n",
      "           1       0.13      0.71      0.22      1463\n",
      "           2       0.25      0.69      0.36      1708\n",
      "           3       0.20      0.40      0.26      4750\n",
      "\n",
      "    accuracy                           0.47     38204\n",
      "   macro avg       0.37      0.56      0.36     38204\n",
      "weighted avg       0.75      0.47      0.54     38204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [500, 1000, 1500], 'max_features': ['auto', 'sqrt'], 'max_depth': [5, 10, 15, 20, None], 'min_samples_split': [2, 5], 'min_samples_leaf': [2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 500, stop = 1500, num = 3)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 20, num = 4)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "source": [
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
