{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./TrainingData/\"\n",
    "TEST_PATH = \"./TestData/\"\n",
    "val_subject = 5\n",
    "test_subject = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for filename in os.listdir(PATH):\n",
    "    files.append(filename)\n",
    "files = sorted(files, key = lambda x: (int(x.split('_')[1]),int(x.split('_')[2]), x.split('_')[4] ))\n",
    "files_train = list(filter(lambda x: int(x.split('_')[1]) not in  [val_subject, test_subject], files))\n",
    "files_val = list(filter(lambda x: int(x.split('_')[1]) == val_subject, files))\n",
    "files_test = list(filter(lambda x: int(x.split('_')[1])== test_subject, files))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinating features and timestamp, by default down sample the train frequency to 10hz by only considering one entry out of 4, If up sample find the nearest label and replace the nan value while merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train(files,X,Y,freq_down=True):\n",
    "    for i in range(0, len(files), 4):\n",
    "        x_time, x, y_time, y = files[i: i + 4]\n",
    "        train_df = None\n",
    "        if freq_down:\n",
    "            #X_features\n",
    "            x_time_df = pd.read_csv(PATH + x_time , header=None)\n",
    "            x_df = pd.read_csv(PATH + x , header=None)\n",
    "            x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
    "            x_combined = x_combined.loc[range(1,len(x_combined), 4)].reset_index()  # down sampled the frequency\n",
    "\n",
    "            #Y_labels\n",
    "            y_time_df = pd.read_csv(PATH + y_time , header=None)\n",
    "            y_df = pd.read_csv(PATH + y , header=None)\n",
    "            y_combined = pd.concat([y_time_df, y_df], axis=1, ignore_index=True)\n",
    "\n",
    "            train_df = pd.concat([x_combined, y_combined], axis=1, ignore_index=True)\n",
    "            train_df = train_df.drop(columns=[0, 1, 8])  # Dropping the time stamp\n",
    "            Y.extend(train_df[9].values)\n",
    "            X.extend(train_df.drop(columns=[9]).values)\n",
    "        else:\n",
    "            #X_features\n",
    "            x_time_df = pd.read_csv(PATH + x_time, header=None)\n",
    "            x_time_df.astype('float64')\n",
    "            x_df = pd.read_csv(PATH + x, header=None)\n",
    "            x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
    "            x_combined = x_combined.rename({0:'timestamp'}, axis='columns')\n",
    "            x_combined.set_index('timestamp', inplace=True)\n",
    "\n",
    "            #Y_labels\n",
    "            y_time_df = pd.read_csv(PATH + y_time , header=None)\n",
    "            y_time_df.astype('float64')\n",
    "            y_df = pd.read_csv(PATH + y, header=None)\n",
    "            y_combined = pd.concat([y_time_df, y_df], axis=1, ignore_index=True)\n",
    "            y_combined = y_combined.rename({0:'timestamp'}, axis='columns')\n",
    "            y_combined.set_index('timestamp', inplace=True)\n",
    "            train_df = pd.merge_asof(left=x_combined, right=y_combined, left_index=True, right_index = True, direction='nearest')  # Merging using the neareset values\n",
    "            X.extend(train_df.drop(columns=['1_y']).values)\n",
    "            Y.extend(train_df['1_y'].values)\n",
    "        \n",
    "    print(len(X), len(Y))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1341646 1341646\n",
      "1053327 1053327\n",
      "135503 135503\n",
      "152816 152816\n",
      "Dataset Seperation Done\n"
     ]
    }
   ],
   "source": [
    "X_total, y_total = [], []\n",
    "make_train(files, X_total, y_total, freq_down=False)\n",
    "X_train, y_train = [], []\n",
    "make_train(files_train, X_train, y_train,freq_down=False)\n",
    "X_val, y_val = [], []\n",
    "make_train(files_val, X_val, y_val,freq_down=False)\n",
    "X_test, y_test = [], []\n",
    "make_train(files_test, X_test, y_test,freq_down=False)\n",
    "print(\"Dataset Seperation Done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not really required now, because testing is done using a specific subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
    "                                                    stratify=Y, \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.3403173111841434, 1: 5.9620483155225505, 2: 4.4457683346839545, 3: 1.4949799595785267}\n"
     ]
    }
   ],
   "source": [
    "weight  = compute_class_weight(class_weight = 'balanced', classes = np.unique(y_train), y = np.array(y_train))\n",
    "weight_dict = {}\n",
    "for index, weight in enumerate(weight):\n",
    "    weight_dict[index] = float(weight)\n",
    "print(weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 773783, 1: 44168, 2: 59232, 3: 176144})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 200,  bootstrap = True, random_state = 42, class_weight=weight_dict).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_2 = RandomForestClassifier(n_estimators = 1000, max_depth = 12, max_features = 'auto', bootstrap = True, random_state = 42, class_weight=weight_dict).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./model/rf_200', 'wb+') as f:\n",
    "    pickle.dump(clf,f)\n",
    "\n",
    "with open('./model/rf_1000_12', 'wb+') as f:\n",
    "    pickle.dump(clf_2,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: Counter({0: 124996, 2: 6143, 3: 3055, 1: 1309}), Actual: Counter({0: 112011, 3: 11292, 2: 7004, 1: 5196})\n"
     ]
    }
   ],
   "source": [
    "print(f'Predictions: {Counter(pred)}, Actual: {Counter(y_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91    112011\n",
      "           1       0.47      0.12      0.19      5196\n",
      "           2       0.64      0.56      0.60      7004\n",
      "           3       0.16      0.04      0.07     11292\n",
      "\n",
      "    accuracy                           0.83    135503\n",
      "   macro avg       0.53      0.42      0.44    135503\n",
      "weighted avg       0.78      0.83      0.79    135503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_val, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [500, 1000, 1500], 'max_features': ['auto', 'sqrt'], 'max_depth': [5, 10, 15, 20, None], 'min_samples_split': [2, 5], 'min_samples_leaf': [2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 500, stop = 1500, num = 3)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 20, num = 4)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "source": [
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"./TestData/\"\n",
    "target_path = \"./output/random_forest/\"\n",
    "test_overlap = 30\n",
    "test_files = sorted(os.listdir(test_path))\n",
    "for i in range(0, len(test_files), 3):\n",
    "    X = []\n",
    "    x_time, x, y_time = test_files[i: i + 3]\n",
    "    train_df = None\n",
    "    \n",
    "    #X_features\n",
    "    x_time_df = pd.read_csv(test_path + x_time , header=None)\n",
    "    x_df = pd.read_csv(test_path + x , header=None)\n",
    "    x_combined = pd.concat([x_time_df, x_df], axis=1, ignore_index=True)\n",
    "    x_combined = x_combined.loc[range(1,len(x_combined), 4)].reset_index()  # down sampled the frequency\n",
    "    y_time_df = pd.read_csv(test_path + y_time , header=None)\n",
    "\n",
    "    # train_df = pd.concat([x_combined, y_combined], axis=1, ignore_index=True)\n",
    "    train_df = x_combined.drop(columns=[0, 1])  # Dropping the time stamp\n",
    "    X = np.array(train_df.values)\n",
    "    pred = []\n",
    "    pred = clf.predict(X)\n",
    "    y_df = pd.DataFrame(pred)\n",
    "    filename = path.join(target_path,(x_time.split('__')[0] + '__y.csv'))\n",
    "    y_df.to_csv(filename, index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(kernel=\"linear\", class_weight=weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./model/svm_imbalanced', 'wb+') as f:\n",
    "    pickle.dump(svm_clf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
